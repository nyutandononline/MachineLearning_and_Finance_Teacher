{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multinomial classification\n",
    "A Multinomial Classifier (when categories/classes $||C|| > 2$) can be created from multiple Binary Classifiers\n",
    "- Create a separate Binary Classifier for each $c \\in C$\n",
    "- The classifier for category $c$ attempts to classify\n",
    "    - Each example with target category of $c$ as Positive\n",
    "    - All other examples as Negative\n",
    "- Combine the $||C||$ classifiers to produce a vector $\\hat{p}$ of length $||C||$\n",
    "    - normalize across $c \\in C$ to sum to $1$\n",
    "    - $\\hat{p}_c$ denotes the normalized value for category $c$\n",
    "        - **Notation abuse**: subscripts should be integers, not categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **This is not something you need to do for yourself**\n",
    "    - Built into `sklearn`\n",
    "    - \"All classifiers in scikit-learn do multiclass classification out-of-the-box.\"\n",
    "    - sklearn.multiclass.OneVsRestClassifier(estimator) if you want to create your own binary `estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross Entropy: Loss function for multinomial classification\n",
    "\n",
    "Both the target $\\y$ and the prediction $\\hat{p}$ are represented as vectors of length $||C||$\n",
    "- We write $\\y_c, \\hat{p}_c$ to denote the element of the vector corresponding to category $c$\n",
    "- Each vector can be interpretted as a probability distribution, e.g.\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\forall c \\in C: \\y_c \\ge 0 \\\\\n",
    "\\sum_{c \\in C} { \\y_c } =1\n",
    "\\end{array}\n",
    "$$\n",
    "    - $\\y$  was created with One Hot Encoding (OHE), so properties satisfied by consruction\n",
    "    - $\\hat{p}_c$ satisfies the properties by virtue of the normalization of the predictions of the $||C||$ binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With $\\y, \\hat{p}$ encoded as a vectors, per example Binary Cross Entropy can be generalized to $||C|| \\ge 2$ categories:\n",
    "\n",
    "$$\n",
    "\\loss^\\ip_\\Theta = - \\sum_{c=1}^{||C||}\n",
    "\\left(\n",
    "{ \\y_c^\\ip * \\textrm{log} (\\hat{\\mathbf{p}}_c^\\ip)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This is the multinomial analog of Binary Cross Entropy and is called **Cross Entropy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cross Entropy can be interpretted as a measure of the \"distance\" between distributions $\\y$ and $\\hat{p}$\n",
    "- Minimized when they are identical\n",
    "- We will use Cross Entropy in the future both as a Loss function and a way of comparing probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accuracy is *not* a loss function\n",
    "\n",
    "Recall the mapping of probability to prediction\n",
    "\n",
    "$$\n",
    "\\hat{y}^\\ip = \n",
    "\\left\\{\n",
    "    {\n",
    "    \\begin{array}{lll}\n",
    "     \\text{Negative} & \\textrm{if } \\hat{p}^\\ip   < 0.5  \\\\\n",
    "     \\text{Positive}& \\textrm{if } \\hat{p}^\\ip \\ge 0.5 \n",
    "    \\end{array}\n",
    "    }\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The prediction for example $i$ changes only when probability $\\hat{p}^\\ip$ crosses the threshold.\n",
    "\n",
    "Suppose the class for example $i$ is Positive: $\\y^\\ip = \\text{Positive}$.\n",
    "- Is our model \"better\" when \n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\hat{p}^\\ip &  \\approx 1 & \\text{ than when  }       &\\hat{p}^\\ip  = 0.5\\\\\n",
    "\\hat{p}^\\ip &  = (.5 - \\epsilon) & \\text{ than when  } & \\hat{p}^\\ip  \\approx 0\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The per-example Accuracy is the same in both comparisons\n",
    "- But a model with probability $\\hat{p}^\\ip$ as close to $\\y^\\ip =1$ would seem to better\n",
    "\n",
    "There is no *degree* or magnitude of inaccuracy\n",
    "- Two models may have the same Accuracy even though the probabilities of one may be closer to perfect than the other\n",
    "- In our search for the best $\\Theta$, Accuracy won't be a guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap of week 3\n",
    "\n",
    "- Good news\n",
    "    - You now know two main tasks in Supervised Learning\n",
    "        - Regression, Classification\n",
    "    - You now know how to use virtually every model in `sklearn`\n",
    "        - Consistent API\n",
    "            - `fit`, `transform`, `predict`\n",
    "    - You survived the \"sprint\" to get you up and running with ML\n",
    "    - You know the *mechanical process* to implement transformations: Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Even better news\n",
    "    - There's a lot more !\n",
    "        - Machine Learning is about *problem solving* **not** using an API\n",
    "\n",
    "This week we start to focus on how to become an effective problem solver\n",
    "- Error Analysis\n",
    "- Deeper Understanding of Loss functions\n",
    "- Transformations in more depth\n",
    "\n",
    "All of this is in service of how you can improve models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
