{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3.7",
      "language": "python",
      "name": "python37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Sentiment_from_tweets.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9306e427a41192b7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "epPOoVgHL6k6",
        "colab_type": "text"
      },
      "source": [
        "# Problem description\n",
        "\n",
        "Another type of less traditional data is text.\n",
        "There is potentially a lot of information about a company in documents such as\n",
        "- News articles\n",
        "- Annual/quarterly filings\n",
        "- Analyst reports\n",
        "- Blogs\n",
        "\n",
        "The key element about text is that a document is a *sequence* of words.\n",
        "In other words, order matters.\n",
        "Consider\n",
        "- \"Machine Learning is easy not hard\"\n",
        "- \"Machine Learning is hard not easy\"\n",
        "\n",
        "Two sentences with identical words but different meaning.\n",
        "\n",
        "In this assignment we will analyze text in the form of Tweets.\n",
        "Our objective is: given a tweet about a company, does the tweet indicate Positive sentiment or Negative sentiment.\n",
        "\n",
        "This assignment will also serve as a preview of Natural Language Processing: the use of Machine Learning to analyze text.\n",
        "This will be the subject of a later lecture.\n",
        "\n",
        "Our immediate objective is to use Recurrent Neural Networks to analyze a sequence of words (i.e., a tweet).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-78900cbcab792210",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "c4v8SsonL6k7",
        "colab_type": "text"
      },
      "source": [
        "## Goal: problem set 1\n",
        "\n",
        "There are two notebook files in this assignment:\n",
        "- **`Sentiment_from_tweets.ipynb`**: First and only notebook you need to work on. Train your models and save them\n",
        "- **`Model_test.ipynb`**: Test your results. After you complete the `Ships_in_satellite_images_P2.ipynb`, this notebook should be submitted\n",
        "\n",
        "**Before you start working on this assignment, please change your kernel to Python 3.7**\n",
        "\n",
        "In this `Sentiment_from_tweets.ipynb` notebook, you will need to create Sequential models in Keras to analyze the sentiment in tweets.\n",
        "- Each example is a sequence of words\n",
        "- The labels are integers: high values indicate Positive sentiment, low values indicate Negative sentiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7821570da816e179",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "wOv4_ZsAL6k7",
        "colab_type": "text"
      },
      "source": [
        "## Learning objectives\n",
        "- Learn how to use Recurrent Layer types as part of a Keras Sequential model\n",
        "- Appreciate how layer choices impact number of weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-fcc034f8649f2d7d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zm-IkmH4L6k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "## Import tensorflow and check the version of tensorflow\n",
        "import tensorflow as tf\n",
        "print(\"Running TensorFlow version \",tf.__version__)\n",
        "\n",
        "# Parse tensorflow version\n",
        "version_match = re.match(\"([0-9]+)\\.([0-9]+)\", tf.__version__)\n",
        "tf_major, tf_minor = int(version_match.group(1)) , int(version_match.group(2))\n",
        "print(\"Version {v:d}, minor {m:d}\".format(v=tf_major, m=tf_minor) )\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "import IPython\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scz7Ve67L6k_",
        "colab_type": "text"
      },
      "source": [
        "# API for students\n",
        "\n",
        "We will define some utility routines.\n",
        "\n",
        "This will simplify problem solving\n",
        "\n",
        "More importantly: it adds structure to your submission so that it may be easily graded\n",
        "\n",
        "**If you want to take a look at the API, you can open it by selecting \"File->Open->RNN_helper.py\"**\n",
        "\n",
        "`helper = RNN_helper.rnn_helper()`\n",
        "\n",
        "### Preprocess raw dataset\n",
        "- getDataRaw: get raw data. \n",
        "  >`DIR` is the directory of data     \n",
        "  >`tweet_file` is the name of data file     \n",
        "  >`tweets_raw = helper.getDataRaw(DIR, tweet_file)`   \n",
        "- getTextClean: clean text. \n",
        "  >`tweets_raw` is the raw data you get from `helper.getDataRaw()`, which is a pandas DataFrame     \n",
        "  >`docs, sents = helepr.getTextClean(tweets_raw)`     \n",
        "- show: display data by reversing index back to word. \n",
        "  >`tok` is an object of `Tokenizer`     \n",
        "  >`encoded_docs_padded` is the text data which you have encoded and padded      \n",
        "  >`helper.show(tok, encoded_docs_padded)`      \n",
        "- getExamples: one-hot encode samples. \n",
        "  >`encoded_docs_padded` is the text data which you have encoded and padded     \n",
        "  >`sents` is the labels     \n",
        "  >`max_features` is number of words in the vocabulary    \n",
        "  >`X, y = helper.getExamples(encoded_docs_padded, sents, max_features)`\n",
        "  \n",
        "### Save model and load model\n",
        "- save model (portable): save a model in `./models` directory\n",
        "  >`helper.saveModel(model, modelName)`\n",
        "- save history (non-portable): save a model history in `./models` directory\n",
        "  >`helper.saveHistory(history, modelName)`\n",
        "- save model (portable): save a model in `./models` directory\n",
        "  >`helper.saveModel(model, modelName)`\n",
        "- save history (non-portable): save a model history in `./models` directory\n",
        "  >`helper.saveHistory(history, modelName)`\n",
        "\n",
        "### Plot models and training results\n",
        "- plotModel: plot your models\n",
        "  >`plotModel(model, model_name)`\n",
        "- plot_training: plot your training results\n",
        "  >`plot_training(history, metric='acc)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0845f25fb1b50a0a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Zhr9q6SDL6lA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# Reload all modules imported with %aimport\n",
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "\n",
        "import RNN_helper\n",
        "%aimport RNN_helper\n",
        "\n",
        "helper = RNN_helper.rnn_helper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d54bac1a7319634e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xkUYjUYmL6lC",
        "colab_type": "text"
      },
      "source": [
        "## Get the tweets (as text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0a7e779edf946a67",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tr4_ltiTL6lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory and file name\n",
        "DATA_DIR = \"../resource/asnlib/publicdata/tweets/Data\"\n",
        "tweet_file = \"Apple-Twitter-Sentiment-DFE-1.csv\"\n",
        "\n",
        "# Load raw data\n",
        "tweets_raw = helper.getDataRaw(DATA_DIR, tweet_file)\n",
        "tweets_raw[ [\"text\", \"sentiment\"] ].head(10)\n",
        "\n",
        "print(\"Sentiment values (raw)\", np.unique(tweets_raw[\"sentiment\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f6544931cb72f5db",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "k1U5uQi9L6lG",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "There will be a number of preprocessing steps necessary to convert the raw tweets to a form\n",
        "amenable to a Neural Network.\n",
        "\n",
        "The next few cells will take you through the journey from **\"raw\" data** to the **X** (array of examples)\n",
        "and **y** (array of labels for each example) arrays that you will need for your Neural Network.\n",
        "\n",
        "In an academic setting you will often be given X and y.\n",
        "This will rarely be the case in the real world.\n",
        "\n",
        "So although this journey has little to do with our objective in learning about Recurrent Neural Networks,\n",
        "we encourage you to follow along.\n",
        "\n",
        "If you are anxious to get to the Recurrent Neural Network part: you can defer the journey until later\n",
        "and skip to the cell that defines X and y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-58f7cd24f1089a9d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "X__w6n79L6lH",
        "colab_type": "text"
      },
      "source": [
        "As you can see, tweets have their own special notation that distinguishes it from ordinary language\n",
        "- \"Mentions\" begin with \"@\" and refer to another user: \"@kenperry\"\n",
        "- \"Hash tags\" begin witn \"#\" and refer to a subject: #MachineLearning\n",
        "\n",
        "This means that our vocabulary (set of distinct words) can be huge.  To manage the vocabulary size\n",
        "and simplify the problem (perhaps losing information on the way), we will **not** distinguish between\n",
        "individual mentions and hash tags\n",
        "\n",
        "Let's also examine the possible sentiment values\n",
        "- There is a \"not_relevant\" value; we should eliminate these examples\n",
        "- The sentiment value is a string\n",
        "- The strings represent non-consecutive integers\n",
        "\n",
        "There is quite a bit of cleaning of the raw data necessary; fortunately, we will do that for you below. We will use `helper.getTextClean()` here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2b742d3dae30e3bc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "mYtHsW7xL6lH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs, sents = helper.getTextClean(tweets_raw)\n",
        "\n",
        "print(\"Docs shape is \", docs.shape)\n",
        "print(\"Sents shape is \", sents.shape)\n",
        "\n",
        "print(\"Possible sentiment values: \",  np.unique(sents) ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8c5216fdfdcf568d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WPWpOcHsL6lJ",
        "colab_type": "text"
      },
      "source": [
        "## More data preprocessing\n",
        "\n",
        "Great, our text is in much better shape and our sentiment (target value for prediction) are now consecutive values.\n",
        "\n",
        "But computers handle numbers much more readily than strings.\n",
        "We will need to convert the text in a *sequence* of numbers\n",
        "- Break text up into words\n",
        "- Assign each word a distinct integer\n",
        "\n",
        "Moreover, it will be easier if all sequences have the same length.\n",
        "We can add a \"padding\" character to the front if necessary.\n",
        "\n",
        "Again, we do this for you below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-96b3933bc03c4d67",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "mTmWKVuxL6lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "## set parameters\n",
        "# max_features: number of words in the vocabulary (and hence, the length of the One Hot Encoding feature vector)\n",
        "# maxlen: number of words in a review\n",
        "max_features = 1000\n",
        "maxlen = 40\n",
        "\n",
        "## Tokenize text\n",
        "tok = Tokenizer(num_words=max_features)\n",
        "tok.fit_on_texts(docs)\n",
        "\n",
        "encoded_docs = tok.texts_to_sequences(docs)\n",
        "# The length of different sequence samples may be different, so we use padding method to make them have same length\n",
        "encoded_docs_padded = sequence.pad_sequences(encoded_docs, maxlen=maxlen)\n",
        "\n",
        "encoded_docs_padded[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f4db56388d013230",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-EwNSADeL6lM",
        "colab_type": "text"
      },
      "source": [
        "## Verify that our encoded documents are the same as the cleaned original\n",
        "\n",
        "At this point: convince yourself that all we have done was encode words as integers and pad out all text to the same length.  The following will demonstrate this. We will use `helper.show()` here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "pcHqM3UAL6lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "helper.show(tok, encoded_docs_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-96075d0a37f290bb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "j7X6Qw6CL6lP",
        "colab_type": "text"
      },
      "source": [
        "## Even more preprocessing\n",
        "\n",
        "Although a word has been encoded as an integer, this integer doesn't have a particular meaning.\n",
        "\n",
        "We will therefore convert each word to a One Hot Encoded (OHE) vector\n",
        "- The length of the vector is equal to the length of the vocabulary (set of distinct words)\n",
        "- The vector is all 0 except for a single location which will be 1\n",
        "- If the word is the $k^{th}$ word of the vocabulary, the position of the sole 1 will be $k$\n",
        "\n",
        "This representation is called One Hot Encoding\n",
        "- A word as a feature vector of length $V$, where $V$ is the number of words in the vocabulary\n",
        "    - Feature $j$ is a binary indicator which is true if the word is the $j^{th}$ word in the vocabulary\n",
        "    \n",
        "Finally: we can get the set of examples and associated labels in a form ready for processing by\n",
        "the Neural Network.\n",
        "\n",
        "At this point, they will be hard to recognize by a human being. We will use `helper.getExamples()` here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-54550db5e697910c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "rBLTVNn2L6lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = helper.getExamples(encoded_docs_padded, sents, max_features)\n",
        "print(X[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1103753ac6019cac",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ugxCGuLZL6lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save X and y for further testing\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "np.savez_compressed('./data/dataset.npz', X = X, y = y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bdd9e38f88bd6e50",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5pmcl3OJL6lU",
        "colab_type": "text"
      },
      "source": [
        "## A note on the representation of words as OHE vectors\n",
        "\n",
        "There are *much better* representations of words than as OHE vectors !\n",
        "\n",
        "We will learn about this in our lecture on Natural Languag Processing.\n",
        "\n",
        "For now, the OHE representation will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ae0cdb28af6194b6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "N1ALgjrnL6lU",
        "colab_type": "text"
      },
      "source": [
        "# Split the data into test and training sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0855eed8c1847068",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "OapiUsgdL6lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bdd86029cc4a895f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "dUMp-SMuL6lW",
        "colab_type": "text"
      },
      "source": [
        "# How long is the sequence in a *single* example\n",
        "\n",
        "**Question:** Compute the length and number of features of a sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "How_long_sequence",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "AZwuUFQsL6lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set two variables\n",
        "# example_sequence_len: length of the sequence\n",
        "# example_num_features: number of features in a single element of the sequence (of a single example)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "example_shape = X.shape[1:]\n",
        "example_sequence_len, example_num_features = example_shape[0], example_shape[1]\n",
        "### END SOLUTION\n",
        "\n",
        "print('The length of a sequence is ', example_sequence_len)\n",
        "print('Number of features is ', example_num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "x63BevIiL6lZ",
        "colab_type": "text"
      },
      "source": [
        "Your length of sequence should be the **maxlen** you set  \n",
        "Your number of features should be the **max_features** you set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-6b2fd158d79f5b83",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SHC-OgI-L6lZ",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Create a Keras Sequential model using a Recurrent layer type\n",
        "\n",
        "You will create a model that\n",
        "- takes as input: a sequence of one hot encodings of words (i.e., a representation of a tweet)\n",
        "- predicts (outputs) a sentiment\n",
        "\n",
        "**Note**\n",
        "You should treat the sentiment as a Categorical (discrete) value, rather than a continous one\n",
        "- As we saw: the sentiment label values are not continuous\n",
        "- We cannot really assign a \"magnitude\" to the sentiment\n",
        "    - We cannot say that a sentiment of 5 is five times \"higher\" than a sentiment of 1\n",
        "- We will thus treat the problem as one of Classification rather than Regression\n",
        "- **We have not one hot encoded the labels** (i.e., the sents variable). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4821aa9d61df6585",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "fsb8iae3L6la",
        "colab_type": "text"
      },
      "source": [
        "## Create model \n",
        "\n",
        "**Question:** Build a very basic model with two layers\n",
        "- A Recurrent layer (LSTM to be specific) with a hidden state size of 128, name it \"lstm_1\"\n",
        "- A Head layer implementing multinomial Classification, name it \"dense_head\"\n",
        "\n",
        "**Hint:**\n",
        "Since this is a multi-classification problem, you need to use `softmax` function for your head layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "build_simple_RNN",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "oxhkHAhkL6la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_lstm = None\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "model_lstm = Sequential( [LSTM(128, input_shape=X.shape[-2:], name='lstm_1'),\n",
        "                          Dense( len( np.unique(y)), name='dense_head', activation=\"softmax\")\n",
        "                         ]\n",
        "                       )\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a5f931ed29245c59",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7A3pNGB1L6ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot your model\n",
        "plot_lstm = helper.plotModel(model_lstm, \"lstm\")\n",
        "IPython.display.Image(plot_lstm) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-91c09cacc73249ab",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "phXSCdcvL6lg",
        "colab_type": "text"
      },
      "source": [
        "## Train model\n",
        "\n",
        "**Question:** Now that you have built your first RNN model, next you will compile and train your model first way. The base requirements are as follows:\n",
        "- Name your model \"LSTM_sparsecat\"\n",
        "- Split your dataset `X_train` into 0.9 training data and 0.1 validation data. Set the `random_state` to be 42. You can use `train_test_split()`\n",
        "- Metric: accuracy\n",
        "- Training epochs is 15\n",
        "- Save your training results in a variable named `history1`\n",
        "- Plot your training results using API `helper.plot_training()`\n",
        "\n",
        "**Note: about loss function**   \n",
        "- Like what we mentioned before, we haven't one-hot encoded our lables, so please use `sparse_categorical_crossentropy()` as loss function and here. \n",
        "- Alternatively, you can encode the labels using `to_categorical()` and use `categorical_crossentropy` as loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMiA4issL6lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameters\n",
        "model_name_1 = 'LSTM_sparsecat'\n",
        "num_epochs = 15\n",
        "\n",
        "# If you don't use one-hot encoded labels\n",
        "loss_ = 'sparse_categorical_crossentropy'\n",
        "metric = 'acc'\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "X_train_, X_val_, y_train_, y_val_ = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
        "model_lstm.compile(loss=loss_,\n",
        "            metrics=[metric]\n",
        "            )\n",
        "\n",
        "history1 = model_lstm.fit(X_train_, y_train_,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=(X_val_, y_val_))\n",
        "\n",
        "# Plot training result\n",
        "helper.plot_training(history1)\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-XVECQqL6lj",
        "colab_type": "text"
      },
      "source": [
        "**Expected outputs (there may be some differences):**  \n",
        "<table> \n",
        "    <tr> \n",
        "        <td>  \n",
        "            Training accuracy\n",
        "        </td>\n",
        "        <td>\n",
        "         0.9120\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr> \n",
        "        <td>\n",
        "            Validation accuracy\n",
        "        </td>\n",
        "        <td>\n",
        "         0.7259\n",
        "        </td>\n",
        "    </tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "The loss and accuracy graphs of first model are similiar to this:\n",
        "<img src=\"https://github.com/nyutandononline/MachineLearning_and_Finance_Teacher/blob/master/Assignment_DL_teacher/DL_3_sentiment_from_tweets/images/model1_acc.png?raw=1\" style=\"width:600px;height:300px;\">\n",
        "<img src=\"https://github.com/nyutandononline/MachineLearning_and_Finance_Teacher/blob/master/Assignment_DL_teacher/DL_3_sentiment_from_tweets/images/model1_loss.png?raw=1\" style=\"width:600px;height:300px;\">\n",
        "\n",
        "We can see that the accuracy curve in the graph above seems going up and down while the training curve is increasing, which means our model may have a overfitting problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "NZF4nwH_L6lj",
        "colab_type": "text"
      },
      "source": [
        "## Evalutate the model\n",
        "\n",
        "**Question:** We have trained our model, then what we need to do next is to evaluate the model using test dataset. Please store the model score in a variable named `score1`.   \n",
        "**Hint:** The method we should use is `evaluate()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "bpCgjed4L6lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "socre1 = None\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "score1 = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
        "### END SOLUTION\n",
        "\n",
        "print(\"{n:s}: Test loss: {l:3.2f} / Test accuracy: {a:3.2f}\".format(n=model_name_1, l=score1[0], a=score1[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bc98d98ca6b84a08",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "VBELanQXL6lm",
        "colab_type": "text"
      },
      "source": [
        "## Save the trained model_lstm and history1 for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-24697f2c2819849c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "i2dmm4UtL6lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "helper.saveModel(model_lstm, model_name_1)\n",
        "helper.saveHistory(history1, model_name_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-db0ba37a6cf03aca",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tvVV67vSL6lp",
        "colab_type": "text"
      },
      "source": [
        "## Let's check the number of our models, how many weights in your recurrent model?\n",
        "\n",
        "How many weights in your model?\n",
        "\n",
        "You should always be sensitive to how \"big\" your model is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-abdeff68cd17a9d4",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xNW1fQg0L6lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set variable\n",
        "# - num_weights_lstm: number of weights in your model\n",
        "num_weights_lstm = 0\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "num_weights_lstm = model_lstm.count_params()\n",
        "### END SOLUTION\n",
        "\n",
        "print(\"number of parameters is \", num_weights_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-00ae8f085cb69e3a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Y8fQ-bZxL6lr",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Create a model consider only of a Classification head\n",
        "\n",
        "The Recurrent layer type creates a fixed length (i.e., size of hidden state) encoding of a variable length input sequence\n",
        "- No matter how long the input, the encoding will have fixed length\n",
        "\n",
        "But it needs quite a few parameters, and seems to have a overfitting problem.\n",
        "\n",
        "Let's compare this to a simple Classifier only model\n",
        "- That reduces the sequence to a single feature vector\n",
        "    - Length of the single feature vector is the same as any element of the sequence\n",
        "- There are a couple of ways to do this\n",
        "    - Take the sum or average (across the sequence) of each feature\n",
        "    - Take the max (across the sequence) of each feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b8a182d09cf81f9f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3KyZFNxxL6lr",
        "colab_type": "text"
      },
      "source": [
        "**Question:** Create a Keras Sequential model (only 1 pooling layer + Head layer) that\n",
        "- Reduces the sequence to a singleton with the same number of features\n",
        "- Classifies directly on this singleton\n",
        "- Name your head layer \"dense_head\"\n",
        "\n",
        "**Hint:**\n",
        "- Investigate the Keras `GlobalMaxPooling1D` and `GlobalAveragePooling1D` layer types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "build_model_only_head_layer",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "QMUqHEdrL6ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_simple = None\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "model_simple = Sequential( [ GlobalMaxPooling1D(input_shape=X_train.shape[-2:]),\n",
        "                             Dense( len( np.unique(y) ), activation=\"softmax\", name='dense_head')\n",
        "                           ]\n",
        "                         )\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "# Plot model\n",
        "plot_simple = helper.plotModel(model_simple, \"simple\")\n",
        "IPython.display.Image(plot_simple) \n",
        "\n",
        "model_simple.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8ce545d82f6cf31b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "YcKR7hp5L6lu",
        "colab_type": "text"
      },
      "source": [
        "## Train model\n",
        "\n",
        "**Question:** Now that you have built your Classification model, next you will compile and train your model. The base requirements are as follows:\n",
        "- Name your model \"Only_head\"\n",
        "- Split your dataset `X_train, y_train` into 0.9 training data and 0.1 validation data. Set the `random_state` to be 42. You can use `train_test_split()`\n",
        "- Metric: \"accuracy\"; loss function: \"sparse_categorical_crossentropy\" (don't use the one-hot encoded labels)\n",
        "- Training epochs is 15\n",
        "- Save your training results in a variable named `history2`\n",
        "- Plot your training results using API `plot_training()`\n",
        "\n",
        "**loss function:** Do not one-hot encode labels, use `sparse_categorical_crossentropy` as loss function  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "train_only_head_layer_model",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "scrolled": true,
        "id": "x6wZZHzcL6lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameters\n",
        "model_name_2 = 'Only_head'\n",
        "num_epochs = 15\n",
        "metric = 'acc'\n",
        "loss_ = 'sparse_categorical_crossentropy'\n",
        "\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# X_train_, X_val_, y_train_, y_val_ = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
        "model_simple.compile(loss= loss_,\n",
        "            metrics=[metric]\n",
        "            )\n",
        "\n",
        "history2 = model_simple.fit(X_train_, y_train_,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=(X_val_, y_val_))\n",
        "\n",
        "# Plot training result\n",
        "helper.plot_training(history2)\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a3b9136c0a04ff84",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IxF_niizL6lw",
        "colab_type": "text"
      },
      "source": [
        "**Expected outputs (there may be some differences):**  \n",
        "<table> \n",
        "    <tr> \n",
        "        <td>  \n",
        "            Training accuracy\n",
        "        </td>\n",
        "        <td>\n",
        "         0.7776\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr> \n",
        "        <td>\n",
        "            Validation accuracy\n",
        "        </td>\n",
        "        <td>\n",
        "         0.7784\n",
        "        </td>\n",
        "    </tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "The loss and accuracy graphs of first model are similiar to this:\n",
        "<img src=\"https://github.com/nyutandononline/MachineLearning_and_Finance_Teacher/blob/master/Assignment_DL_teacher/DL_3_sentiment_from_tweets/images/model2_acc.png?raw=1\" style=\"width:600px;height:300px;\">\n",
        "<img src=\"https://github.com/nyutandononline/MachineLearning_and_Finance_Teacher/blob/master/Assignment_DL_teacher/DL_3_sentiment_from_tweets/images/model2_loss.png?raw=1\" style=\"width:600px;height:300px;\">\n",
        "\n",
        "We can see that two accuracy curves in the graph above are increasing, which means our model is learning even though it is very simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5b024575e8ea3398",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WlvZADyuL6lw",
        "colab_type": "text"
      },
      "source": [
        "## Save the trained model_simple and history2 for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3324fdffb7eb218b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1RG6nphRL6lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "helper.saveModel(model_simple, model_name_2)\n",
        "helper.saveHistory(history2, model_name_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f1feed2553377a71",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "r8t3kvOUL6l0",
        "colab_type": "text"
      },
      "source": [
        "## How many weights in your Classifier only model ?\n",
        "\n",
        "How many weights in your model ?\n",
        "\n",
        "You should always be sensitive to how \"big\" your model is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ad84e21e2763f573",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aT6WSFm4L6l0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set variable\n",
        "# - num_weights_lstm: number of weights in your model\n",
        "num_weights_simple = 0\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "num_weights_simple = model_simple.count_params()\n",
        "### END SOLUTION\n",
        "\n",
        "print(\"number of parameters is \", num_weights_simple)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-80490d1c1d149b60",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "NfAqULskL6l3",
        "colab_type": "text"
      },
      "source": [
        "Compared with the previous RNN moddel, we have much **less** parameters, but the validation accuracy is better than RNN model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0ad6ed9cd2b135fb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "CeeuNHC8L6l3",
        "colab_type": "text"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "- Was the increase in number of weights compensated by a gain in accuracy when using a Recurrent Layer type compared to the Classifier only model ?\n",
        "- Can you speculate why this is so ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i86L2kxmL6l3",
        "colab_type": "text"
      },
      "source": [
        "## Now Submit your assignment!\n",
        "Please click on the blue button <span style=\"color: blue;\"> **Submit** </span> in this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5TklJGUL6l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}