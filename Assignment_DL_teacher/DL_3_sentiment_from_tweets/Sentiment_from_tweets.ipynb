{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9306e427a41192b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem description\n",
    "\n",
    "Another type of less traditional data is text.\n",
    "There is potentially a lot of information about a company in documents such as\n",
    "- News articles\n",
    "- Annual/quarterly filings\n",
    "- Analyst reports\n",
    "- Blogs\n",
    "\n",
    "The key element about text is that a document is a *sequence* of words.\n",
    "In other words, order matters.\n",
    "Consider\n",
    "- \"Machine Learning is easy not hard\"\n",
    "- \"Machine Learning is hard not easy\"\n",
    "\n",
    "Two sentences with identical words but different meaning.\n",
    "\n",
    "In this assignment we will analyze text in the form of Tweets.\n",
    "Our objective is: given a tweet about a company, does the tweet indicate Positive sentiment or Negative sentiment.\n",
    "\n",
    "This assignment will also serve as a preview of Natural Language Processing: the use of Machine Learning to analyze text.\n",
    "This will be the subject of a later lecture.\n",
    "\n",
    "Our immediate objective is to use Recurrent Neural Networks to analyze a sequence of words (i.e., a tweet).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78900cbcab792210",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Goal: problem set 1\n",
    "\n",
    "In this notebook, you will need to create Sequential models in `Tensorflow/Keras` to analyze the sentiment in tweets.\n",
    "- Each example is a sequence of words\n",
    "- The labels are integers: high values indicate Positive sentiment, low values indicate Negative sentiment\n",
    "\n",
    "\n",
    "There are two notebook files in this assignment:\n",
    "- The one you are viewing now: First and only notebook you need to work on. \n",
    "    - Train your models here\n",
    "    - There are cells that will save your models to a file\n",
    "- **`Model_test.ipynb`**:\n",
    "    - This notebook is used to grade your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7821570da816e179",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning objectives\n",
    "- Learn how to construct Neural Networks in a Keras Sequential model that uses Recurrent layer types.\n",
    "- Appreciate how layer choices impact number of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcc034f8649f2d7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Import tensorflow and check the version of tensorflow\n",
    "import tensorflow as tf\n",
    "print(\"Running TensorFlow version \",tf.__version__)\n",
    "\n",
    "# Parse tensorflow version\n",
    "version_match = re.match(\"([0-9]+)\\.([0-9]+)\", tf.__version__)\n",
    "tf_major, tf_minor = int(version_match.group(1)) , int(version_match.group(2))\n",
    "print(\"Version {v:d}, minor {m:d}\".format(v=tf_major, m=tf_minor) )\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import IPython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06894ddd3930b8e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# API for students\n",
    "\n",
    "We have defined some utility routines in a file `RNN_helper.py`. There is a class named `rnn_helper` in it.  \n",
    "\n",
    "This will simplify problem solving\n",
    "\n",
    "More importantly: it adds structure to your submission so that it may be easily graded\n",
    "\n",
    "**If you want to take a look at the API, you can open it by selecting \"File->Open->RNN_helper.py\"**\n",
    "\n",
    "`helper = RNN_helper.rnn_helper()`\n",
    "\n",
    "### Preprocess raw dataset\n",
    "- getDataRaw: get raw data.    \n",
    "  >`tweets_raw = helper.getDataRaw()`   \n",
    "- getTextClean: clean text. \n",
    "  >`tweets_raw` is the raw data you get from `helper.getDataRaw()`, which is a pandas DataFrame     \n",
    "  >`docs, sents = helepr.getTextClean(tweets_raw)`     \n",
    "- show: display data by reversing index back to word. \n",
    "  >`tok` is an object of `Tokenizer`     \n",
    "  >`encoded_docs_padded` is the text data which you have encoded and padded      \n",
    "  >`helper.show(tok, encoded_docs_padded)`      \n",
    "- getExamples: one-hot encode samples. \n",
    "  >`encoded_docs_padded` is the text data which you have encoded and padded     \n",
    "  >`sents` is the labels     \n",
    "  >`max_features` is number of words in the vocabulary    \n",
    "  >`X, y = helper.getExamples(encoded_docs_padded, sents, max_features)`\n",
    "  \n",
    "### Save model and load model\n",
    "- save model: save a model in `./models` directory\n",
    "  >`helper.saveModel(model, modelName)`\n",
    "- save history: save a model history in `./models` directory\n",
    "  >`helper.saveHistory(history, modelName)`\n",
    "- load model: load a model in `./models` directory\n",
    "  >`helper.loadModel(modelName)`\n",
    "- load history: load a model history in `./models` directory\n",
    "  >`helper.loadHistory(modelName)`\n",
    "\n",
    "### Plot models and training results\n",
    "- plotModel: plot your models\n",
    "  >`plotModel(model, model_name)`\n",
    "- plot_training: plot your training results\n",
    "  >`plot_training(history, metric='acc)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0845f25fb1b50a0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import RNN_helper\n",
    "%aimport RNN_helper\n",
    "\n",
    "helper = RNN_helper.rnn_helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d54bac1a7319634e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Get the tweets (as text)\n",
    "\n",
    "The first step in our Recipe is Get the Data.\n",
    "\n",
    "We have provided a utility method `getDataRaw` to simplify this for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a7e779edf946a67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "tweets_raw = helper.getDataRaw()\n",
    "tweets_raw[ [\"text\", \"sentiment\"] ].head(10)\n",
    "\n",
    "print(\"Sentiment values (raw)\", np.unique(tweets_raw[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6544931cb72f5db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "There will be a number of preprocessing steps necessary to convert the raw tweets to a form\n",
    "amenable to a Neural Network.\n",
    "\n",
    "The next few cells will take you through the journey from **\"raw\" data** to the **X** (array of examples)\n",
    "and **y** (array of labels for each example) arrays that you will need for your Neural Network.\n",
    "\n",
    "In an academic setting you will often be given X and y.\n",
    "This will rarely be the case in the real world.\n",
    "\n",
    "So although this journey has little to do with our objective in learning about Recurrent Neural Networks,\n",
    "we encourage you to follow along.\n",
    "\n",
    "If you are anxious to get to the Recurrent Neural Network part: you can defer the journey until later\n",
    "and skip to the cell that defines X and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58f7cd24f1089a9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, tweets have their own special notation that distinguishes it from ordinary language\n",
    "- \"Mentions\" begin with \"@\" and refer to another user: \"@kenperry\"\n",
    "- \"Hash tags\" begin witn \"#\" and refer to a subject: #MachineLearning\n",
    "\n",
    "This means that our vocabulary (set of distinct words) can be huge.  To manage the vocabulary size\n",
    "and simplify the problem (perhaps losing information on the way), we will **not** distinguish between\n",
    "individual mentions and hash tags\n",
    "\n",
    "Let's also examine the possible sentiment values\n",
    "- There is a \"not_relevant\" value; we should eliminate these examples\n",
    "- The sentiment value is a string\n",
    "- The strings represent non-consecutive integers\n",
    "\n",
    "There is quite a bit of cleaning of the raw data necessary; fortunately, we will do that for you below. We will use `helper.getTextClean()` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b742d3dae30e3bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "docs, sents = helper.getTextClean(tweets_raw)\n",
    "\n",
    "print(\"Docs shape is \", docs.shape)\n",
    "print(\"Sents shape is \", sents.shape)\n",
    "\n",
    "print(\"Possible sentiment values: \",  np.unique(sents) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c5216fdfdcf568d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## More data preprocessing\n",
    "\n",
    "Great, our text is in much better shape and our sentiment (target value for prediction) are now consecutive values.\n",
    "\n",
    "But computers handle numbers much more readily than strings.\n",
    "We will need to convert the text in a *sequence* of numbers\n",
    "- Break text up into words\n",
    "- Assign each word a distinct integer\n",
    "\n",
    "Moreover, it will be easier if all sequences have the same length.\n",
    "We can add a \"padding\" character to the front if necessary.\n",
    "\n",
    "Again, we do this for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96b3933bc03c4d67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Set parameters\n",
    "#  max_features: number of words in the vocabulary (and hence, the length of the One Hot Encoding feature vector)\n",
    "#  maxlen: number of words in a review\n",
    "max_features = 1000\n",
    "maxlen = 40\n",
    "\n",
    "## Tokenize text\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(docs)\n",
    "\n",
    "encoded_docs = tok.texts_to_sequences(docs)\n",
    "# The length of different sequence samples may be different, so we use padding method to make them have same length\n",
    "encoded_docs_padded = sequence.pad_sequences(encoded_docs, maxlen=maxlen)\n",
    "\n",
    "encoded_docs_padded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4db56388d013230",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Verify that our encoded documents are the same as the cleaned original\n",
    "\n",
    "At this point: convince yourself that all we have done was encode words as integers and pad out all text to the same length.  \n",
    "\n",
    "The following will demonstrate this. We will use `helper.show()` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c39b21d45989ac16",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "helper.show(tok, encoded_docs_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96075d0a37f290bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Even more preprocessing\n",
    "\n",
    "Although a word has been encoded as an integer, this integer doesn't have a particular meaning.\n",
    "\n",
    "We will therefore convert each word to a One Hot Encoded (OHE) vector\n",
    "- The length of the vector is equal to the length of the vocabulary (set of distinct words)\n",
    "- The vector is all 0 except for a single location which will be 1\n",
    "- If the word is the $k^{th}$ word of the vocabulary, the position of the sole 1 will be $k$\n",
    "\n",
    "This representation is called One Hot Encoding\n",
    "- A word as a feature vector of length $V$, where $V$ is the number of words in the vocabulary\n",
    "    - Feature $j$ is a binary indicator which is true if the word is the $j^{th}$ word in the vocabulary\n",
    "    \n",
    "Finally: we can get the set of examples and associated labels in a form ready for processing by\n",
    "the Neural Network.\n",
    "\n",
    "At this point, they will be hard to recognize by a human being. We will use `helper.getExamples()` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54550db5e697910c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = helper.getExamples(encoded_docs_padded, sents, max_features)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1103753ac6019cac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Save X and y for final testing\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "np.savez_compressed('./data/dataset.npz', X = X, y = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdd9e38f88bd6e50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## A note on the representation of words as OHE vectors\n",
    "\n",
    "There are *much better* representations of words than as OHE vectors !\n",
    "\n",
    "We will learn about this in our lecture on Natural Language Processing.\n",
    "\n",
    "For now, the OHE representation will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae0cdb28af6194b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Split the data into test and training sets\n",
    "\n",
    "To train and evaluate a model, we need to split the original dataset into\n",
    "a training subset (in-sample) and a test subset (out of sample).\n",
    "\n",
    "We will do this for you in the cell below.\n",
    "\n",
    "**DO NOT** shuffle the data until after we have performed the split into train/test sets\n",
    "- We want everyone to have the **identical** test set for grading\n",
    "- Do not change this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0855eed8c1847068",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdd86029cc4a895f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# How long is the sequence in a *single* example\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Compute the length and number of features of a sequence\n",
    "\n",
    "Set the following variables to values as described in the cell below\n",
    "- `example_sequence_len`\n",
    "- `example_num_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "How_long_sequence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set two variables\n",
    "# example_sequence_len: length of the sequence\n",
    "# example_num_features: number of features in a single element of the sequence (of a single example)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "example_shape = X.shape[1:]\n",
    "example_sequence_len, example_num_features = example_shape[0], example_shape[1]\n",
    "### END SOLUTION\n",
    "\n",
    "print('The length of a sequence is ', example_sequence_len)\n",
    "print('Number of features is ', example_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c2b1ba1c679ecd7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Hopefully, you answered the question by examining the examples.\n",
    "\n",
    "For comparison: the cell in the \"More data preprocessing\" section defined\n",
    "- `maxlen`: the maximum sequence length allowed\n",
    "- `max_features`: the maximum number of features allowed (i.e., vocabulary size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b2fd158d79f5b83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1: Create a Keras Sequential model using a Recurrent layer type\n",
    "\n",
    "You will create a model that\n",
    "- takes as input: a sequence of one hot encodings of words (i.e., a representation of a tweet)\n",
    "- predicts (outputs) a sentiment\n",
    "\n",
    "**Note**\n",
    "You should treat the sentiment as a Categorical (discrete) value, rather than a continuous one\n",
    "- As we saw: the sentiment label values are not continuous\n",
    "- We cannot really assign a \"magnitude\" to the sentiment\n",
    "    - We cannot say that a sentiment of 5 is five times \"higher\" than a sentiment of 1\n",
    "- We will thus treat the problem as one of Classification rather than Regression\n",
    "- **We have not one hot encoded the labels** (i.e., the `sents` variable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4821aa9d61df6585",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Create model \n",
    "\n",
    "**Question:** \n",
    "\n",
    "Build a very basic model with two layers\n",
    "- A Recurrent layer (LSTM to be specific) with a hidden state size of `NUM_HIDDEN_STATES` (as set below), name it \"lstm_1\"\n",
    "- A Head layer implementing multinomial Classification, name it \"dense_head\"\n",
    "\n",
    "Set variable `model_lstm` to be a Keras `Sequential` model object that implements your model.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Since this is a multi-classification problem, you need to use `softmax` function for your head layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "build_simple_RNN",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set model_lstm equal to a Keras Sequential model\n",
    "model_lstm = None\n",
    "NUM_HIDDEN_STATES = 128 # Hidden state size\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model_lstm = Sequential( [LSTM(NUM_HIDDEN_STATES, input_shape=X.shape[-2:], name='lstm_1'),\n",
    "                          Dense( len( np.unique(y)), name='dense_head', activation=\"softmax\")\n",
    "                         ]\n",
    "                       )\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5f931ed29245c59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot your model\n",
    "plot_lstm = helper.plotModel(model_lstm, \"lstm\")\n",
    "IPython.display.Image(plot_lstm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91c09cacc73249ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Train model\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Now that you have built your first model, you will compile and train it. The requirements are as follows:\n",
    "\n",
    "- Name your model \"LSTM_sparsecat\" and store it in variable `model_name_1`\n",
    "- Split the **training** examples `X_train, y_train` again !\n",
    "    - 90% will be used for training the model\n",
    "    - 10% will be used as validation (out of sample) examples\n",
    "    - Use `train_test_split()` from `sklearn` to perform this split\n",
    "        -  Set the `random_state` parameter of `train_test_split()` to be 42\n",
    "\n",
    "- Loss function: `sparse_categorical_crossentropy`\n",
    "- Metric: \"accuracy\"\n",
    "- Use the value in variable `max_epochs` as the number of epochs for training\n",
    "- Save your training results in a variable named `history1`\n",
    "- Plot your training results using API `helper.plot_training()`\n",
    "\n",
    "**Loss functions:**  `sparse_categorical_crossentropy` versus `categorical_crossentropy` \n",
    "- `categorical_crossentropy` computes Cross Entropy loss when targets are One Hot Encoded\n",
    "- `sparse_categorical_crossentropy` computes Cross Entropy loss when targets are consecutive integers\n",
    "- We have **not** used OHE for our labels here, so don't use `categorical_crossentropy`\n",
    "- Alternatively, you can encode the labels using `to_categorical()` and use `categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee57d62153c4f5f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the name and max_epochs of model_lstm\n",
    "model_name_1 = 'LSTM_sparsecat'\n",
    "max_epochs = 15\n",
    "\n",
    "# If you don't use one-hot encoded labels\n",
    "loss_ = 'sparse_categorical_crossentropy'\n",
    "metric = 'acc'\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X_train_, X_val_, y_train_, y_val_ = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
    "model_lstm.compile(loss=loss_,\n",
    "            metrics=[metric]\n",
    "            )\n",
    "\n",
    "history1 = model_lstm.fit(X_train_, y_train_,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(X_val_, y_val_))\n",
    "\n",
    "# Plot training result\n",
    "helper.plot_training(history1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a056e1ecce2b83b",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Evalutate the model\n",
    "\n",
    "**Question:**\n",
    "\n",
    "We have trained our model. We now need to  evaluate the model using the test dataset created in an earlier cell.\n",
    "\n",
    "Please store the model score in a variable named `score1`.   \n",
    "\n",
    "**Hint:** \n",
    "\n",
    "The model object has a method  `evaluate`.  Use that to compute the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-958049fa4270d9c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "socre1 = []\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "score1 = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"{n:s}: Test loss: {l:3.2f} / Test accuracy: {a:3.2f}\".format(n=model_name_1, l=score1[0], a=score1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc98d98ca6b84a08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Save the trained model_lstm and history1 for submission\n",
    "\n",
    "Your fitted model can be saved for later use\n",
    "- In general: so you can resume training at a later time\n",
    "- In particular: to allow us to grade it !\n",
    "\n",
    "Execute the following cell to save your model, which you will submit to us for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-24697f2c2819849c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "helper.saveModel(model_lstm, model_name_1)\n",
    "helper.saveHistory(history1, model_name_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db0ba37a6cf03aca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Let's check the number of our models, how many weights in your recurrent model?\n",
    "\n",
    "**Question:**\n",
    "\n",
    "How many weights in your model? You should always be sensitive to how \"big\" your model is.\n",
    "\n",
    "- Set `num_weights_lstm` to be equal to the number of weights in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abdeff68cd17a9d4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set variable\n",
    "# - num_weights_lstm: number of weights in your model\n",
    "num_weights_lstm = 0\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "num_weights_lstm = model_lstm.count_params()\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"number of parameters is \", num_weights_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00ae8f085cb69e3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 2: Create a model consisting only of a Classification head\n",
    "\n",
    "The Recurrent layer type creates a fixed length (i.e., size of hidden state) encoding of a variable length input sequence\n",
    "- No matter how long the input, the encoding will have fixed length\n",
    "\n",
    "But it needs quite a few parameters, and seems to have a overfitting problem.\n",
    "\n",
    "Let's compare this to a simple Classifier only model\n",
    "- That reduces the sequence to a single feature vector\n",
    "    - Length of the single feature vector is the same as any element of the sequence\n",
    "- There are a couple of ways to do this\n",
    "    - Take the sum or average (across the sequence) of each feature\n",
    "    - Take the max (across the sequence) of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8a182d09cf81f9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question:** \n",
    "\n",
    "Create a Keras Sequential model\n",
    "- Using only two layers:\n",
    "    - 1 pooling layer\n",
    "    - A Classification head\n",
    "        - Name your head layer \"dense_head\"\n",
    "- Set variable `model_simple` to be a Keras `Sequential` model object that implements your model.\n",
    "\n",
    "The pooling layer should reduce your variable length sequence to a fixed length.\n",
    "- That's what the RNN layer of the previous model did, although in a more complex manner.\n",
    "\n",
    "A fixed length input is what the Classification head needs.\n",
    "\n",
    "**Hint:**\n",
    "- Investigate the Keras `GlobalMaxPooling1D` and `GlobalAveragePooling1D` layer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "build_model_only_head_layer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set model_simple equal to a Keras Sequential model\n",
    "model_simple = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model_simple = Sequential( [ GlobalMaxPooling1D(input_shape=X_train.shape[-2:]),\n",
    "                             Dense( len( np.unique(y) ), activation=\"softmax\", name='dense_head')\n",
    "                           ]\n",
    "                         )\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# Plot model\n",
    "plot_simple = helper.plotModel(model_simple, \"simple\")\n",
    "IPython.display.Image(plot_simple) \n",
    "\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ce545d82f6cf31b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Train model\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Train your new model following the same instructions as given for training the first model.\n",
    "- **Except**: Save your training results in a variable named `history2`\n",
    "- Name your model \"Only_head\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "train_only_head_layer_model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_name_2 = 'Only_head'\n",
    "max_epochs = 15\n",
    "metric = 'acc'\n",
    "loss_ = 'sparse_categorical_crossentropy'\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "model_simple.compile(loss= loss_,\n",
    "            metrics=[metric]\n",
    "            )\n",
    "\n",
    "history2 = model_simple.fit(X_train_, y_train_,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(X_val_, y_val_))\n",
    "\n",
    "# Plot training result\n",
    "helper.plot_training(history2)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b024575e8ea3398",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Save the trained model_simple and history2 for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3324fdffb7eb218b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "helper.saveModel(model_simple, model_name_2)\n",
    "helper.saveHistory(history2, model_name_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1feed2553377a71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## How many weights in your Classifier only model ?\n",
    "\n",
    "**Question:** \n",
    "\n",
    "How many weights in your model ? You should always be sensitive to how \"big\" your model is.\n",
    "- Set `num_weights_simple` to be equal to the number of parameters in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad84e21e2763f573",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set variable\n",
    "#  num_weights_simple number of weights in your model\n",
    "num_weights_simple = 0\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "num_weights_simple = model_simple.count_params()\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"number of parameters is \", num_weights_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80490d1c1d149b60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Compared with the previous RNN moddel, we have much **less** parameters, but the validation accuracy is better than RNN model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ad6ed9cd2b135fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "- Was the increase in number of weights compensated by a gain in accuracy when using a Recurrent Layer type compared to the Classifier only model ?\n",
    "- Can you speculate why this is so ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Submit your assignment!\n",
    "Please click on the blue button <span style=\"color: blue;\"> **Submit** </span> in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
