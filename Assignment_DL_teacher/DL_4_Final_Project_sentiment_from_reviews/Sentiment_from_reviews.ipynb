{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yN3g4v8Zl4Fy",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01c6fc97fb1c251c",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Final Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moP0TbbABpSP",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b540779b49f3d87",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem description\n",
    "\n",
    "Time to show off everything you learned !\n",
    "\n",
    "You will be performing a Classification task to analyze the sentiment of product reviews.\n",
    "\n",
    "This is similar to a prior assignment\n",
    "- With a different dataset\n",
    "- Multinomial classification with 5 classes\n",
    "\n",
    "But, by now, you have many more tools at your disposal.\n",
    "\n",
    "## Some possible approaches\n",
    "- A review is a sequence of words.  You will need to deal with sequences in some manner.  Some suggestions\n",
    "  - Pooling\n",
    "  - Recurrent Neural Network\n",
    "- Is there an advantage to recognizing *adjacent* words groups (\"n-grams\") rather than treating the document as an unordered set of words ?\n",
    "  - Consider these two sentences\n",
    "    - \"Machine Learning is easy not hard\"\n",
    "    - \"Machine Learning is hard not easy\"\n",
    "\n",
    "  - Two sentences with identical words but different meaning.\n",
    "  - Hint: Convolutional layer\n",
    "- How should we encode words ?\n",
    "  - OHE ? Embedding ?\n",
    "\n",
    "We will **not specify** an approach.  Feel free to experiment.\n",
    "\n",
    "Your goal is to produce a model with an out of sample accuracy meeting a minimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "# Advice \n",
    "- Your first model should be *simple* (e.g., OHE + GlobalMaxPooling + Logistic Regression)\n",
    "    - Use it to study the data and get a feel for the problem\n",
    "    - It establishes a baseline from which to improve\n",
    "- Use Error Analysis to understand where your model is failing.\n",
    "    - Perhaps the failure cases suggest improvements ?\n",
    "- Remember: this is an *iterative* process\n",
    "    - Your later models can become increasingly complex (e.g., Embedding + LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIA71lpFmaw8"
   },
   "source": [
    "## Grading\n",
    "\n",
    "Prior assignments evaluated you step by step.\n",
    "\n",
    "This project is **results-based**. Your goal is to create a model\n",
    "- That achieves an out of sample accuracy of at least 50%\n",
    "- 60% would be better !\n",
    "\n",
    "There are three data files in this directory:  \n",
    "- `train.csv`:\n",
    "    - This is the dataset on which you will train your model\n",
    "- `test.csv`:\n",
    "    - This is the dataset by which you will be judged !\n",
    "    - It has no labels so **you** can't use it to train or test your model\n",
    "        - But **we do have** the labels so we can test your accuracy\n",
    "    - Once you have built your model, you will make predictions on these examples and submit them for grading\n",
    "- `submit_sample.csv`:\n",
    "    - The file of predictions that you will submit should be similar in format to this file\n",
    "\n",
    "**The file that you submit for grading**\n",
    "- Should be named \"my_submit.csv\"\n",
    "**Submit your file: save outputs of your model in a pandas dataframe, name it \"my_submit.csv\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEqGfYbUBpSQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc5c6e8e0ded5670",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Learning objectives\n",
    "- Experimentation !\n",
    "- Error Analysis leading to model improvement\n",
    "- Appreciate how choices impact number of weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efiTwPiKBpSc",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbec58f97c332aa8",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TensorFlow version  2.0.0\n",
      "Version 2, minor 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Running TensorFlow version \",tf.__version__)\n",
    "\n",
    "# Parse tensorflow version\n",
    "version_match = re.match(\"([0-9]+)\\.([0-9]+)\", tf.__version__)\n",
    "tf_major, tf_minor = int(version_match.group(1)) , int(version_match.group(2))\n",
    "print(\"Version {v:d}, minor {m:d}\".format(v=tf_major, m=tf_minor) )\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bmNajQkBpSf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bb179ecfb795fea",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# API for students\n",
    "\n",
    "We will define some utility routines.\n",
    "\n",
    "This will simplify problem solving\n",
    "\n",
    "More importantly: it adds structure to your submission so that it may be easily graded\n",
    "\n",
    "**If you want to take a look at the API or change it, you can open it by selecting \"File->Open->final_helper.py\"**\n",
    "\n",
    "`helper = final_helper.HELPER()`\n",
    "\n",
    "## Preprocess raw dataset\n",
    "- getDataRaw: get raw data. \n",
    "  >`DIR` is the directory of data        \n",
    "  >`tweet_file` is the name of data file          \n",
    "  >`tweets_raw = helper.getDataRaw(DIR, tweet_file)`         \n",
    "- getTextClean: clean text. \n",
    "  >`data_raw` is the raw data you get from `helper.getDataRaw()`, which is a pandas DataFrame          \n",
    "  >`textAttr` is the column of text data      \n",
    "  >`sentAttr` is the column of label       \n",
    "  >`docs, sents = helepr.getTextClean(data_raw, textAttr, sentAttr`         \n",
    "- encodeDocs: text tokenization\n",
    "  >`docs` is the text data          \n",
    "  >`vocab_size` is the size of vocabulary           \n",
    "  >`words_in_doc` is number of words in a review           \n",
    "  >`tok, encoded_docs, encoded_docs_padded = helper.encodeDocs(docs, vocab_size, words_in_doc)`        \n",
    "- showEncodedDocs: display data by reversing index back to word. \n",
    "  >`tok` is an object of `Tokenizer`             \n",
    "  >`encoded_docs_padded` is the text data which you have encoded and padded                  \n",
    "  >`helper.showEncodedDocs(tok, encoded_docs_padded)`                   \n",
    "- getExamplesOHE: one-hot encode samples. \n",
    "  >`encoded_docs_padded` is the text data which you have encoded and padded                 \n",
    "  >`sents` is the labels                \n",
    "  >`vocab_size` is number of words in the vocabulary           \n",
    "  >`X, y = helper.getExamples(encoded_docs_padded, sents, vocab_size)`          \n",
    "\n",
    "## Train model\n",
    "- trainModelCat: train model for categorical labels\n",
    "  >`patience` and `min_delta` are parameters of `EarlyStopping`        \n",
    "  >`history = helper.trainModelCat(model, X_train, X_val, y_train, y_val, num_epochs=30, metric=\"acc\", patience=5, min_delta=.005)`\n",
    "  \n",
    "## Save model and load model\n",
    "- save model (portable): save a model in `./models` directory\n",
    "  >`helper.saveModel(model, modelName)`\n",
    "- save history (non-portable): save a model history in `./models` directory\n",
    "  >`helper.saveHistory(history, modelName)`\n",
    "- save model (portable): save a model in `./models` directory\n",
    "  >`helper.saveModel(model, modelName)`\n",
    "- save history (non-portable): save a model history in `./models` directory\n",
    "  >`helper.saveHistory(history, modelName)`\n",
    "\n",
    "## Plot models and training results\n",
    "- plotModel: plot your models\n",
    "  >`helper.plotModel(model, model_name)`\n",
    "- plot_training: plot your training results\n",
    "  >`helper.plot_training(history, metric='acc)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiwvVQUJBpSV",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-848b28446ea22d6a",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import final_helper\n",
    "%aimport final_helper\n",
    "\n",
    "helper = final_helper.HELPER()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHUZacWsBpSi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-169c6349738edbf2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "\n",
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUhrnGczBpSi"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./Data/train.csv' does not exist: b'./Data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d9f45d46dd21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDataRaw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDataRaw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/NYU/Assignment_DL_teacher/DL_4_Final_Project_sentiment_from_reviews/final_helper.py\u001b[0m in \u001b[0;36mgetDataRaw\u001b[0;34m(self, DIR, data_file, dropAttrs)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetDataRaw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropAttrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropAttrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./Data/train.csv' does not exist: b'./Data/train.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = \"./Data\"\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "train_raw = helper.getDataRaw(DATA_DIR, train_file)\n",
    "test_raw = helper.getDataRaw(DATA_DIR, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "z6x5Q--mJXDE",
    "outputId": "2463af1a-fda4-4288-8616-5d333c772acf"
   },
   "outputs": [],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2XffyPrDedWO",
    "outputId": "14c176e6-c6f4-4820-fc38-0b9f9fb3f625"
   },
   "outputs": [],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xN_lWRgsBpSl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-edfb0c181cc9a0fb",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The reviews are in the \"reviewText\" attribute.\n",
    "\n",
    "\n",
    "\n",
    "You may try to use other attributes as additional features if you choose, but we suggest that your first model may use this as the only source of features.\n",
    "\n",
    "When you are manipulating your training set, **DON'T FORGET** to do the same manipulation on **test** set, because you need to use your test set as input of your final model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0ufUriSrtp6",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-528c54eb82babc0b",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Get the labelled training data\n",
    "- Features: docs.  Each document is a single review (sequence of characters)\n",
    "- Targets/Labels: sents. Each is the sentiment associated with the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "vwoaUwe-BpSm",
    "outputId": "ff9004ae-0baf-4341-b343-f66d647ffe06"
   },
   "outputs": [],
   "source": [
    "textAttr, sentAttr, titleAttr = \"reviewText\", \"overall\", \"title\"\n",
    "\n",
    "## Clean text\n",
    "# training data\n",
    "docs, sents = helper.getTextClean(train_raw, textAttr, sentAttr)\n",
    "\n",
    "# We will treat the sentiment values as Categorical, rather than numeric\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "sents = le.fit_transform(sents)\n",
    "\n",
    "print(\"Docs shape is \", docs.shape)\n",
    "print(\"Sents shape is \", sents.shape)\n",
    "\n",
    "print(docs[:5])\n",
    "print(\"\\nPossible sentiment values: \",  np.unique(sents) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJyBBa-PBpSo",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a726be25f6aa8ad",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## More data preprocessing\n",
    "\n",
    "We will need to convert the text in a *sequence* of numbers\n",
    "- Break text up into words\n",
    "- Assign each word a distinct integer\n",
    "\n",
    "Moreover, it will be easier if all sequences have the same length.\n",
    "We can add a \"padding\" character to the front if necessary.\n",
    "\n",
    "We do this for you below.\n",
    "\n",
    "Our method returns\n",
    "- encoded_docs_padded: A matrix of training example *features*\n",
    "  - Each row is an example\n",
    "  - Each row is a *sequence* of fixed length\n",
    "  - Each element of the sequence is an integer, encoding a word in the vocabulary\n",
    "  - The sequence length of every example is *identical* because we have prepended padding if necessary\n",
    "- encoded_docs: A matrix of *unpadded* training example *features*\n",
    "- tok: the Tokenizer used to\n",
    "  - parse strings of characters into words\n",
    "  - encoded each word as an integer\n",
    "\n",
    "\n",
    "You may study our methods parameters and modify them if you wish, e.g., alter the size of the vocabulary or length of sequences.\n",
    "\n",
    "We suggest that your first model uses\n",
    "- encoded_docs_padded as your set of training features, e.g., X\n",
    "- sents: as your targets\n",
    "with the default settings of the method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "ggdZME_FBpSo",
    "outputId": "4d3b180b-9337-4374-8772-f5b29e35f196"
   },
   "outputs": [],
   "source": [
    "## set parameters:\n",
    "# vocab_size : number of words in the vocabulary \n",
    "# words_in_doc: number of words in a review\n",
    "vocab_size_sm, words_in_doc_sm = 400, 100\n",
    "\n",
    "tok, encoded_docs, encoded_docs_padded = helper.encodeDocs(docs_total, vocab_size=vocab_size_sm, words_in_doc=words_in_doc_sm)\n",
    "\n",
    "print(\"Training example features shape: \",encoded_docs_padded.shape)\n",
    "print(\"Training example features: preview\")\n",
    "print(encoded_docs_padded[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSPg2B1RBpSq",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2631bcc154a972f",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Verify that our encoded documents are the same as the cleaned original\n",
    "\n",
    "At this point: convince yourself that all we have done was encode words as integers and pad out all text to the same length.  The following will demonstrate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "9HDv4_BXBpSr",
    "outputId": "7a675806-1dd5-4c8b-c0a0-882a67866da4"
   },
   "outputs": [],
   "source": [
    "helper.showEncodedDocs(tok, encoded_docs_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution !\n",
    "\n",
    "How will you encode words ?\n",
    "  \n",
    "- Perhaps you want to use OHE.  If so: we provide some utility functions to help.\n",
    "  >`X_train_OHE, _ = helper.getExamplesOHE(X_train, sents, vocab_size_sm)`       \n",
    "  >`X_val_OHE, _ = helper.getExamplesOHE(X_val, sents, vocab_size_sm)` \n",
    "  \n",
    "  But be **careful**: Our vocabulary is very large.  One Hot Encoding may use too much memory and your program won't run.\n",
    "  \n",
    "- Alternatives to OHE\n",
    " - You can try an embedding layer which is a *dense* representation of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXTltHXxx10g"
   },
   "source": [
    "# Split the examples into training and validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzxNUXNXvu7r"
   },
   "outputs": [],
   "source": [
    "# Set the following variables\n",
    "# - X_train: ndarray of training example features\n",
    "# - X_val:  ndarray of validation example features\n",
    "# - y_train: ndarray of training example targets\n",
    "# - y_val:  ndarray of validation example targets\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(encoded_docs_padded[:11523], sents, test_size=0.10, random_state=42)\n",
    "\n",
    "# OHE the X for some models\n",
    "vocab_size_sm, words_in_doc_sm = 400, 100\n",
    "X_train_OHE, _ = helper.getExamplesOHE(X_train, sents, vocab_size_sm)\n",
    "X_val_OHE, _ = helper.getExamplesOHE(X_val, sents, vocab_size_sm)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6oS5W_hBpS2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0de960d0bf5c6d38",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Verify your training and test dataset\n",
    "\n",
    "# Set two variables\n",
    "# example_sequence_len: length of the sequence\n",
    "# example_num_features: number of features in a single element of the sequence (of a single example)\n",
    "\n",
    "# If using OHE:\n",
    "example_shape = X_train_OHE.shape[1:]\n",
    "example_sequence_len, example_num_features = example_shape[0], example_shape[1]\n",
    "\n",
    "assert example_sequence_len == words_in_doc_sm\n",
    "assert example_num_features == vocab_size_sm\n",
    "\n",
    "# If NOT using OHE\n",
    "example_shape = X_train.shape[1:]\n",
    "example_sequence_len, example_num_features = example_shape[0], tok.num_words\n",
    "\n",
    "assert example_sequence_len == words_in_doc_sm\n",
    "assert example_num_features == vocab_size_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHurBXP607UT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfdf056d0938d0ef",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create and train your model\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- There is a `trainModelCat()` API already in the `final_helper.py` file. You can directly use it by\n",
    "  >`history = helper.trainModelCat(model, X_train, X_val, y_train, y_val, num_epochs=30, metric=\"acc\", patience=5, min_delta=.005)`\n",
    "  \n",
    "  You can change the `trainModelCat()` code or write training process by yourself if you have better idea!   \n",
    "\n",
    "\n",
    "- To to see your model performance, use this API is very convenient\n",
    "  >`helper.plot_training(history)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o97G-ue51CKX"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Set variables\n",
    "# - model: a Keras Sequential model to predict sentiment\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtQ_IGvQBpTS",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1feed2553377a71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## How many weights in your model ?\n",
    "\n",
    "How many weights in your model ?\n",
    "\n",
    "You should always be sensitive to how \"big\" your model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-u1ERSiuBpTS",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad84e21e2763f573",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set variable\n",
    "num_weight = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "num_weights = model.count_params()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iz_Wroa1BpTU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ad6ed9cd2b135fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Submit your predictions for grading\n",
    "\n",
    "Now that you have built your model, it's time to make predictions for grading.\n",
    "- Make a prediction for each example in the file `test.csv`\n",
    "- Create a file `my_submit.csv` with these predictions *in the same order* as the examples in `test.csv`\n",
    "- The format of `my_submit.csv` should be similar to `submit_sample.csv`\n",
    "\n",
    "**Hint**\n",
    "\n",
    "You may want  (but are not required) to use a Pandas DataFrame to create `my_submit.csv`.\n",
    "- Look up the Pandas method `to_csv` in order to create a CSV file from a DataFrame\n",
    "    - Use optional argument `index=False` to prevent line numbers from being inserted in your CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NIgzZKsQ16VX"
   },
   "outputs": [],
   "source": [
    "my_results = pd.DataFrame()\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# test = helper.getDataRaw(DATA_DIR, 'test.csv')\n",
    "# docs = test[textAttr].values\n",
    "\n",
    "# tok, encoded_docs, encoded_docs_padded = helper.encodeDocs(docs, vocab_size=vocab_size_sm, words_in_doc=words_in_doc_sm)\n",
    "test_submit, _ = helper.getExamplesOHE(encoded_docs_padded[11523:], None, vocab_size_sm)\n",
    "\n",
    "my_model = model_simple\n",
    "predicted_class = le.inverse_transform(np.argmax(my_model.predict(test_submit), axis=-1))\n",
    "# predicted_class = le.inverse_transform(np.argmax(my_model.predict(encoded_docs_padded), axis=-1))\n",
    "result = pd.DataFrame({'reviewerID': test_raw['reviewerID'], 'overall':predicted_class})\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "# Save your results in a csv file\n",
    "my_results.to_csv('my_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpWw_tXH_zwu"
   },
   "source": [
    "## Discussion topics\n",
    "- Compare the number of weights in each model.  Did the added complexity (more weights) lead to better performance ?\n",
    "- **Where** were the largest increase in weights between models\n",
    "  - Embeddings consume a lot of weights\n",
    "    - But eliminates a dimension: single integer representation of a word vs a OHE vector of length words_in_vocab\n",
    "    - So subsequent layers may need fewer weights compared to OHE\n",
    "- Should we have formulated this as a Regression task rather than a Classification task ?\n",
    "  - Is the difference in rating between 0 and 1 the same as between 3 and 4 ?\n",
    "    - Perhaps there are bigger *absolute* differences in satisfaction  in lower ratings\n",
    "      - i.e., Big difference between 0 and 1, smaller difference between 3 and 4"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gtQ_IGvQBpTS",
    "IoeQjuCDBpTG"
   ],
   "name": "Sentiment_from_reviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
