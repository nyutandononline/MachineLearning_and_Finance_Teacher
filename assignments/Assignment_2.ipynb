{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Classification\n",
    "\n",
    "You will demonstrate your ability to solve a classification task.\n",
    "\n",
    "The notebook that you submit *should follow the Recipe for Machine Learning* in addition to answering the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "The purpose of this assignment is\n",
    "- to familiarize yourself with Classifiers\n",
    "- to have you explore *feature engineering*\n",
    "\n",
    "We will be using the well-known Titanic challenge, which we introduced in class.\n",
    "\n",
    "Because there are so many solutions to this problem available on the Internet\n",
    "- the base points for this assignment will be lower than usual\n",
    "- your efforts at feature engineering will be a key part of the grade\n",
    "- the *quality of your explanations* will be a big part of the grade\n",
    "    - if your solution is inspired by a model you found elsewhere, your writeup must convince us that you deeply understand all the choices involved\n",
    "        - motivate it by your Exploratory Data Analysis\n",
    "        - try several variations and explain your final choice\n",
    "\n",
    "To make this even more fun, we'll have a contest: extra credit for students whose models\n",
    "perform best on a held-out dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Here's the code to get the data.\n",
    "It has already been split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Note the use of *relative path*; your assignments should all use relative rather than absolute paths\n",
    "TITANIC_PATH = \"./data/assignment_2\"\n",
    "\n",
    "train_data = pd.read_csv( os.path.join(TITANIC_PATH, \"train.csv\") )\n",
    "test_data  = pd.read_csv( os.path.join(TITANIC_PATH, \"test.csv\")  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on the test examples\n",
    "\n",
    "The test examples **do not** have targets (Yes/No for Survived) associated with them,\n",
    "so you can't use these as examples on which to evaluate the Performance Metric.\n",
    "\n",
    "The reason: this problem was part of a competition; the competitors were evaluated on how well\n",
    "they did on the test examples -- the answers were only known to the judges so competitors couldn't cheat.\n",
    "\n",
    "If you want to see how well you predict out of sample\n",
    "- You can choose to create your own test set as a subset of the training examples\n",
    "- You can choose to perform cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a base model using Naive Bayes\n",
    "\n",
    "Use a Naive Bayes classifier as the Base Model in the Recipe.\n",
    "\n",
    "- Report the average of the cross validation scores, using 5 fold cross validation\n",
    "- Use Accuracy as your Performance Metric; report the accuracy\n",
    "\n",
    "**Question**\n",
    "Replace the 0 values in the following cell with your answers, and execute the print statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: Avg cross val score = 0.00\n",
      "Naive Bayes: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = \"Naive Bayes\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=model, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=model, a=accuracy) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Logistic Regression model with minimal feature engineering\n",
    "\n",
    "Create a Logistic Regression classifier using only transformations that are absolutely necessary,\n",
    "for example\n",
    "- dealing with missing features\n",
    "- categorical transformations\n",
    "\n",
    "**Question**\n",
    "Replace the 0 values in the following cell with your answers, and execute the print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, version 0: Avg cross val score = 0.00\n",
      "Logistic Regression, version 0: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = \"Logistic Regression, version 0\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=model, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=model, a=accuracy) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform feature engineering and create another Logistic Regression classifier\n",
    "\n",
    "Use transformations and creation of new features to improve your first Logistic Regression classifier.\n",
    "\n",
    "The first set of  transformations require you to convert `Age` from continuous to buckets/bins.\n",
    "This means choosing how many buckets/bins and what the boundaries are.\n",
    "\n",
    "You will make two different choices for the buckets and report the accuracy of each.\n",
    "You should *clearly explain* why you made the choices that you did (based on logic, Exploratory Data Analysis, etc.).\n",
    "\n",
    "The steps are:\n",
    "- choose a set of buckets and compare your Accuracy out of sample with the first Logistic Regression classifier\n",
    "- choose a *second* set of buckets and compare your Accuracy out of sample with the first Logistic Regression classifier\n",
    "\n",
    "So you will answer two nearly identical questions.  Please report the **best** result in the **second answer**.\n",
    "\n",
    "**Question**\n",
    "Replace the 0 values in the following cell with your answers, and execute the print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, bucketing version 1: Avg cross val score = 0.00\n",
      "Logistic Regression, bucketing version 1: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = \"Logistic Regression, bucketing version 1\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=model, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=model, a=accuracy) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a *different* bucketing scheme for `Age`\n",
    "\n",
    "**Question**\n",
    "Replace the 0 values in the following cell with your answers, and execute the print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, bucketing version 2: Avg cross val score = 0.00\n",
      "Logistic Regression, bucketing version 2: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = \"Logistic Regression, bucketing version 2\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=model, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=model, a=accuracy) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age bucket: categorical or numeric ? \n",
    "\n",
    "Using your best bucketing choice (the second one above)\n",
    "- What is the accuracy when you treat the buckets as numeric ?\n",
    "- What is the accuracy when you treat the buckets as categorical ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, bucketing version 2; buckets treated as numeric features: Avg cross val score = 0.00\n",
      "Logistic Regression, bucketing version 2; buckets treated as numeric features: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = \"Logistic Regression, bucketing version 2; buckets treated as numeric features\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=model, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=model, a=accuracy) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, bucketing version 2; buckets treated as categorical features: Avg cross val score = 0.00\n",
      "Logistic Regression, bucketing version 2; buckets treated as categorical features: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = \"Logistic Regression, bucketing version 2; buckets treated as categorical features\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=model, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=model, a=accuracy) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Extra credit)  Perform more feature engineering\n",
    "\n",
    "We will award extra points for each (up to a maximum of 3) transformation  judged to be well thought out.\n",
    "This means that *you must clearly explain* your ideas, experiments and results.\n",
    "\n",
    "**Question**\n",
    "Replace the 0 values in the following cell with your answers, and execute the print statements\n",
    "- Also replace the \"???\" with a description of your feature engineering.\n",
    "\n",
    "Repeat this cell for each new feature engineering/transformation that you submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???: Avg cross val score = 0.00\n",
      "???: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "title = \"???\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=title, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=title, a=accuracy) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Extra, extra credit) Contest !\n",
    "\n",
    "Come up with your best model for the Titanic !  Use any model and whatever feature engineering you'd like.\n",
    "\n",
    "We will evaluate your model on a held-out dataset.  Top scorers will get extra credit.\n",
    "\n",
    "## Rules\n",
    "- You *may not* include any packages that are not part of the standard installation\n",
    "    - they won't run on the grader's machine\n",
    "- Your feature engineering *must* deal with missing data for all attributes\n",
    "    - the evaluation set *will* have missing values for some features\n",
    "    \n",
    "\n",
    "**Question**\n",
    "Replace the 0 values in the following cell with your answers, and execute the print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical advice\n",
    "\n",
    "The held-out dataset comes from the same distribution as the training examples.\n",
    "\n",
    "But it's possible that there may be some feature\n",
    "- that was *not* missing in any training example\n",
    "- but *is* missing in some example in the held-out dataset\n",
    "\n",
    "Code defensively !  It's not a bad idea to perform missing feature imputation on *all* features, whether\n",
    "or not they are missing in training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic contest: Avg cross val score = 0.00\n",
      "Titanic contest: Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "title = \"Titanic contest\"\n",
    "cross_val_avg = 0\n",
    "\n",
    "print(\"{m:s}: Avg cross val score = {sc:3.2f}\".format(m=title, sc=cross_val_avg) )\n",
    "\n",
    "accuracy = 0\n",
    "print(\"{m:s}: Accuracy = {a:.2%}\".format(m=title, a=accuracy) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
