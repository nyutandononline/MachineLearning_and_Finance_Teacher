{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem description\n",
    "\n",
    "You are to predict whether a company will go bankrupt in the following year, based on financial attributes of the company.\n",
    "\n",
    "Perhaps you are contemplating lending money to a company, and need to know whether the company\n",
    "is in near-term danger of not being able to repay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "In the warm up exercise, we walked you through some of the challenges that you will confront\n",
    "- Messy data\n",
    "- Correlated features\n",
    "- Imbalanced dataset\n",
    "\n",
    "For the Final Project you will create a model, following all the steps in the Recipe, to solve\n",
    "the Bankruptcy prediction task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "- Demonstrate mastery on solving a classification problem and presenting\n",
    "the entire Recipe for Machine Learning process in a notebook.\n",
    "- There will be little explicit direction for this task.\n",
    "- It is meant to be analogous to a pre-interview task that a potential employer might assign\n",
    "to verify your skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Grading\n",
    "Prior assignments evaluated you step by step.\n",
    "\n",
    "This project is results-based. Your goal is to create a well performing model.\n",
    "\n",
    "We will give you some metrics on which your model will be judged.\n",
    "Each metric will have 3 thresholds of increasing value\n",
    "- You will get points for each threshold that your model surpasses\n",
    "\n",
    "There are 2 files:\n",
    "\n",
    "- `train/data.csv`:      \n",
    "    - This is the dataset on which you will train your model\n",
    "    \n",
    "- `holdout/data.csv`:\n",
    "    - This is the dataset by which you will be judged !\n",
    "    - It has no labels so you can't use it to train or test your model\n",
    "        - But **the instructors** do have the labels so we can evaluate your model\n",
    "    - Once you have built your model, you will make predictions on these examples and submit them for grading\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Load the bankruptcy_helper module\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Import bankruptcy_helper module\n",
    "import bankruptcy_helper\n",
    "%aimport bankruptcy_helper\n",
    "\n",
    "helper = bankruptcy_helper.Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# API for students\n",
    "\n",
    "We have defined some utility routines in a file `bankruptcy_helper.py`. There is a class named `Helper` in it.  \n",
    "\n",
    "This will simplify problem solving\n",
    "\n",
    "\n",
    "`helper = bankruptcy_helper.Helper()`\n",
    "\n",
    "\n",
    "\n",
    "- getData: get the training data and holdout data\n",
    "  > `train, holdout = getData()`\n",
    "\n",
    "- plot_attr: Create multiple plots of the distribution of the feature names `attr`, one plot per possible value of target/label `y`\n",
    "  >`helper.plot_attr(X, y, attr, trunc)`       \n",
    "\n",
    "  > `X`: DataFrame of features. Each row is an example          \n",
    "  > `y`: DataFrame/ndarray. Label of each example.,      \n",
    "  > `attr`: string.  Name of feature whose distribution will be plotted      \n",
    "  > `trunc`: Scalar. Optional parameter to truncate distribution at a threshold percentage.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Reminders\n",
    "\n",
    "The data set for this exercise is the same as for the warm up exercise.\n",
    "\n",
    "In the warm up: we flagged potential issues with the data\n",
    "- Numeric values encoded as strings\n",
    "- Examples that have features with missing values\n",
    "- Uneven distribution of examples across target values\n",
    "\n",
    "We also expressed the merit of creating your own out of sample dataset on which to\n",
    "evaluate your model before submitting your results for grading.\n",
    "\n",
    "Also: the holdout data (the examples without labels for which your predictions will be graded) come from the\n",
    "same distribution as the data with labels on which you may train/test.\n",
    "So if there are issues with the training/test data, those same issues may be present in the holdout data.\n",
    "\n",
    "Please think about whether some of the lessons and code from the warm up may be useful here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "**Remember**\n",
    "\n",
    "The holdout data is in the same format as the one we used for training\n",
    "- Except that it has no attribute for the target\n",
    "- So you will need to **perform all the transformations on the holdout data**\n",
    "    - As you did on the training data\n",
    "    - Including turning the string representation of numbers into actual numeric data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Create your own model, using the Recipe for Machine Learning\n",
    "\n",
    "Time for you to continue the Recipe for Machine Learning on your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "#  data: training dataset\n",
    "#  holdout: hold out dataset without target\n",
    "data, holdout = helper.getData()\n",
    "target_attr = 'Bankrupt'\n",
    "\n",
    "# Convert all attributes to numeric\n",
    "### BEGIN SOLUTION\n",
    "non_numeric_cols = data.select_dtypes(exclude=['float', 'int']).columns\n",
    "data[ non_numeric_cols] = data[ non_numeric_cols ].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "### END SOLUTION\n",
    "\n",
    "# Separate the target Bankrupt from all features\n",
    "data, labels = data.drop(columns=[target_attr]), data[target_attr]\n",
    "\n",
    "# Shuffle the data\n",
    "data, labels = sklearn.utils.shuffle(data, labels, random_state=42)\n",
    "\n",
    "# Split data into train and test\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.10, random_state=42)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "You hopefully have conducted multiple experiments, tried several forms of data transformation\n",
    "and used a couple of different algorithms.\n",
    "\n",
    "Now you need to make a choice: which decisions will give you the *best* predictions out of sample ?\n",
    "We will refer to this as your \"best model\".\n",
    "\n",
    "For your best model, using the test set you created, report\n",
    "- Accuracy \n",
    "- Recall\n",
    "- Precision\n",
    "\n",
    "We will evaluate your model using the holdout data.  Grades will be based on\n",
    "the following metrics meeting certain thresholds\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "\n",
    "We will evaluate the metric using 3 increasing values for the threshold\n",
    "- You will get points for each threshold that you surpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM (t=None) avg cross val score=0.9382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjp/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM Accuracy: 91.7%, Recall 0.0%, Precision 0.0%\n",
      "Model: Random Forest (t=None) avg cross val score=0.9412\n",
      "\n",
      "\tRandom Forest Accuracy: 93.2%, Recall 25.0%, Precision 76.9%\n",
      "Model: SVM (t=1) avg cross val score=0.9382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjp/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM Accuracy: 91.7%, Recall 0.0%, Precision 0.0%\n",
      "Model: Logistic (t=1) avg cross val score=0.9350\n",
      "\n",
      "\tLogistic Accuracy: 91.5%, Recall 0.0%, Precision 0.0%\n",
      "Model: SVM (t=10) avg cross val score=0.8727\n",
      "\n",
      "\tSVM Accuracy: 85.7%, Recall 47.5%, Precision 28.4%\n",
      "Model: Logistic (t=10) avg cross val score=0.8755\n",
      "\n",
      "\tLogistic Accuracy: 85.3%, Recall 47.5%, Precision 27.5%\n",
      "Model: SVM (t=12) avg cross val score=0.8485\n",
      "\n",
      "\tSVM Accuracy: 82.6%, Recall 55.0%, Precision 25.0%\n",
      "Model: Logistic (t=12) avg cross val score=0.8450\n",
      "\n",
      "\tLogistic Accuracy: 83.4%, Recall 62.5%, Precision 27.8%\n",
      "Model: SVM (t=13) avg cross val score=0.8321\n",
      "\n",
      "\tSVM Accuracy: 81.3%, Recall 60.0%, Precision 24.5%\n",
      "Model: Logistic (t=13) avg cross val score=0.8266\n",
      "\n",
      "\tLogistic Accuracy: 81.1%, Recall 62.5%, Precision 24.8%\n",
      "Model: SVM (t=15) avg cross val score=0.7975\n",
      "\n",
      "\tSVM Accuracy: 78.4%, Recall 65.0%, Precision 22.4%\n",
      "Model: Logistic (t=15) avg cross val score=0.7897\n",
      "\n",
      "\tLogistic Accuracy: 78.2%, Recall 75.0%, Precision 24.0%\n",
      "Model: SVM (t=18) avg cross val score=0.7422\n",
      "\n",
      "\tSVM Accuracy: 72.2%, Recall 82.5%, Precision 20.6%\n",
      "Model: Logistic (t=18) avg cross val score=0.7311\n",
      "\n",
      "\tLogistic Accuracy: 71.8%, Recall 82.5%, Precision 20.4%\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, classification_report\n",
    "\n",
    "impute_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "\n",
    "## SVM and Random Forest model\n",
    "# logistic_clf = linear_model.LogisticRegression(solver = 'liblinear', max_iter = 10000)\n",
    "svm_clf = SVC(gamma=\"auto\", C=.1)\n",
    "forest_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "r = \"None\"\n",
    "\n",
    "for name, clf in { \"SVM\": svm_clf,\n",
    "                   \"Random Forest\": forest_clf\n",
    "                 }.items():\n",
    "    \n",
    "    pipe = Pipeline([(\"imputer\", impute_transformer), \n",
    "                      (\"model\", clf)\n",
    "                     ]\n",
    "                    )\n",
    "    \n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "    print(\"Model: {m:s} (t={r:s}) avg cross val score={s:3.4f}\\n\".format(m=name, r=r, s=scores.mean()) )\n",
    "\n",
    "    # Out of sample prediction\n",
    "    _= pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # recall_\n",
    "    recall_test = recall_score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "    precision_test = precision_score(y_test,   y_pred, pos_label=1, average=\"binary\")\n",
    "\n",
    "\n",
    "    print(\"\\t{m:s} Accuracy: {a:3.1%}, Recall {r:3.1%}, Precision {p:3.1%}\".format(m=name,\n",
    "                                                                                a=accuracy_test,\n",
    "                                                                                r=recall_test,\n",
    "                                                                                p=precision_test\n",
    "                                                                                )\n",
    "         )\n",
    "\n",
    "    \n",
    "### Models with Dimensionality reduction\n",
    "# Reduce the number of features\n",
    "#    Try other models. For example, PCA\n",
    "# Cost sensitive training\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stand_transformer = StandardScaler()\n",
    "\n",
    "cwt = { 0:1, 1:20 }\n",
    "\n",
    "\n",
    "\n",
    "for r in [ 1, 10, 12, 13, 15, 18]:\n",
    "    cwt = { 0:1, 1:r }\n",
    "    \n",
    "    logistic_clf = linear_model.LogisticRegression(\n",
    "        class_weight = cwt,\n",
    "        solver = 'liblinear', max_iter = 10000)\n",
    "    svm_clf = SVC(class_weight = cwt,\n",
    "              gamma=\"auto\", C=.1)\n",
    "    \n",
    "    for name, clf in { \"SVM\": svm_clf,\n",
    "                       \"Logistic\": logistic_clf\n",
    "                     }.items():\n",
    "\n",
    "        pipe = Pipeline([(\"imputer\", impute_transformer), \n",
    "                         (\"Standardize\", stand_transformer),\n",
    "                         (\"PCA\", PCA(n_components = 20)),\n",
    "                         (\"model\", clf)\n",
    "                         ]\n",
    "                        )\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "        print(\"Model: {m:s} (t={r:d}) avg cross val score={s:3.4f}\\n\".format(m=name, r=r, s=scores.mean()) )\n",
    "\n",
    "        # Out of sample prediction\n",
    "        _= pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        accuracy_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # recall_\n",
    "        recall_test = recall_score(y_test, y_pred, pos_label=1, average=\"binary\")\n",
    "        precision_test = precision_score(y_test,   y_pred, pos_label=1, average=\"binary\")\n",
    "\n",
    "        \n",
    "        print(\"\\t{m:s} Accuracy: {a:3.1%}, Recall {r:3.1%}, Precision {p:3.1%}\".format(m=name,\n",
    "                                                                                    a=accuracy_test,\n",
    "                                                                                    r=recall_test,\n",
    "                                                                                    p=precision_test\n",
    "                                                                                    )\n",
    "             )\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Submission guidelines\n",
    "\n",
    "You will make a prediction for *each example* in the holdout dataset.\n",
    "\n",
    "**Question**\n",
    "- Set a variable `my_predictions` to be a list` or `ndarray`of predictions\n",
    "\n",
    "`my_predictions`[i] (Element $i$ of `my_predictions`) should be your prediction\n",
    "- for the $i^{th}$ holdout example\n",
    "- So \n",
    "    - the length of `my_predictions` must be equal to the number of holdout examples\n",
    "    - the ordering of predictions must be the same as the ordering of holdout examples\n",
    "\n",
    "We will evaluate the performance metrics on `my_predictions` and assign  you a grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic (t=13) avg cross val score=0.8270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set variable\n",
    "#  my_predictions: list/ndarray\n",
    "my_predictions = None\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# It should create an array of predictions; we initialize it to the empty array for convenience\n",
    "my_predictions = []\n",
    "\n",
    "# Relative weight of Bankrupt class to Non Bankrupt class\n",
    "r = 13\n",
    "\n",
    "# Class weights\n",
    "cwt = { 0:1, 1:r }\n",
    "\n",
    "logistic_clf = linear_model.LogisticRegression(\n",
    "    class_weight = cwt,\n",
    "    solver = 'liblinear', max_iter = 10000)\n",
    "name = \"Logistic\"\n",
    "\n",
    "pipe = Pipeline([(\"imputer\", impute_transformer), \n",
    "                 (\"Standardize\", stand_transformer),\n",
    "                 (\"PCA\", PCA(n_components = 20)),\n",
    "                 (\"model\", logistic_clf)\n",
    "                 ]\n",
    "               )\n",
    "\n",
    "scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "print(\"Model: {m:s} (t={r:d}) avg cross val score={s:3.4f}\\n\".format(m=name, r=r, s=scores.mean()) )\n",
    "\n",
    "# Fit the model\n",
    "_= pipe.fit(X_train, y_train)\n",
    "\n",
    "# Out of sample prediction \n",
    "_, X_hold = helper.getData()\n",
    "\n",
    "# transform X_hold\n",
    "non_numeric_cols = X_hold.select_dtypes(exclude=['float', 'int']).columns\n",
    "X_hold[ non_numeric_cols] = X_hold[ non_numeric_cols ].apply(pd.to_numeric, downcast='float', errors='coerce')\n",
    "\n",
    "# predict X_hold\n",
    "y_pred = pipe.predict(X_hold)\n",
    "my_predictions = y_pred\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Illustration of grading\n",
    "\n",
    "The following code illustrates how we will grade your predictions.\n",
    "\n",
    "We suggest that you first try this code on the predictions you make from the test dataset you have created\n",
    "so that you can identify any issues that may arise with the holdout dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# y_hold are the labels associated with the holdout dataset (only the instructors have this)\n",
    "\n",
    "# Evaluate the accuracy of the student predictions\n",
    "accuracy_hold = accuracy_score(y_hold, my_predictions)\n",
    "\n",
    "# Evaluate the Recall and Precision of the student predictions\n",
    "recall_hold = recall_score(y_hold, my_predictions, pos_label=1, average=\"binary\")\n",
    "precision_hold = precision_score(y_hold, my_predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "\n",
    "print(\"\\t{m:s} Accuracy: {a:3.1%}, Recall {r:3.1%}, Precision {p:3.1%}\".format(m=name,\n",
    "                                                                            a=accuracy_hold,\n",
    "                                                                            r=recall_hold,\n",
    "                                                                            p=precision_hold\n",
    "                                                                            )\n",
    "             )\n",
    " \n",
    " \n",
    "## Compare the metrics from the student predictions to some thresholds\n",
    "# Check accuracy\n",
    "assert(accuracy_test > 0.75)\n",
    "\n",
    "# Check recall and precision\n",
    "assert( ( (recall_test  > 0.50) and (precision_test > 0.15) )\n",
    "       or\n",
    "        ( (recall_test  > 0.20) and (precision_test > 0.50) )\n",
    "      )\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "check-accuracy",
     "locked": true,
     "points": "50",
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./Data/5th_yr_with_target.csv' does not exist: b'./Data/5th_yr_with_target.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-555855050030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../resource/asnlib'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bankrupt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./Data/5th_yr_with_target.csv' does not exist: b'./Data/5th_yr_with_target.csv'"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "# load the holdout data with targets\n",
    "DATA_DIR = './Data'\n",
    "file_name = '5th_yr_with_target.csv'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    DATA_DIR = '../resource/asnlib'\n",
    "\n",
    "y_hold = pd.read_csv(os.path.join(DATA_DIR, file_name))['Bankrupt']\n",
    "\n",
    "# accuracy\n",
    "accuracy_hold = accuracy_score(y_hold, my_predictions)\n",
    "\n",
    "# recall & precision\n",
    "recall_hold = recall_score(y_hold, my_predictions, pos_label=1, average=\"binary\")\n",
    "precision_hold = precision_score(y_hold, my_predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "# check accuracy\n",
    "assert(accuracy_hold > 0.75)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "check-recall-precision",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "assert( ( (recall_hold  > 0.50) and (precision_hold > 0.15) )\n",
    "       or\n",
    "        ( (recall_hold  > 0.20) and (precision_hold > 0.50) )\n",
    "      )\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "check-higher-accuracy",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Extra points\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert(accuracy_hold > .80)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_hold, precision_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "check-higher-recall-precision",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Extra points\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert( ( (recall_hold > .60) and (precision_hold > 0.20) )\n",
    "       or\n",
    "        ( (recall_hold  > 0.20) and (precision_hold > 0.60) )\n",
    "      )\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "- Most of the features are expressed as ratios: why is that a good idea ?\n",
    "- Even if you don't understand all of the financial concepts behind the names of the attributes\n",
    "    - You should be able to infer some relationships.  For example, here are some definitions of terms\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "X1   & = & \\frac{\\text{net profit} }{ \\text{total assets} } \\\\\n",
    "X9   & = & \\frac{\\text{sales}     }{ \\text{total assets} } \\\\\n",
    "X23  & = & \\frac{\\text{net profit} }{ \\text{sales} } \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "    - Therefore\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "X23  & = & \\frac{X1}{X9} & \\text{Algebra !}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "    - You might speculate that `net profit` is closely related to `gross profit`\n",
    "        - The difference between \"net\" and \"gross\" is usually some type of additions/subtractions\n",
    "    - Is this theory reflected in which features are most highly correlated with `X1` ?\n",
    "- If you perform dimensionality reduction using PCA (the topic of the Unsupervised Learning lecture)\n",
    "    - PCA is scale sensitive\n",
    "    - If you *don't* scale the features: how many do you need to capture 95% of the variance ?\n",
    "    - If you *do* scale the features: how many do you need to capture 95% of the variance ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
