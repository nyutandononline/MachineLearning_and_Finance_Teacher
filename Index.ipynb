{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Index.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchw2xY05Ghu",
        "colab_type": "text"
      },
      "source": [
        "# Week 0\n",
        "\n",
        "**Plan**\n",
        "- Setting up your learning and programming environment\n",
        "\n",
        "\n",
        "**Getting started**\n",
        "- [Setting up your ML environment](Setup_EdX.ipynb)\n",
        "    - [Choosing an ML environment](Choosing_an_ML_Environment.ipynb)\n",
        "- [Quick intro to the tools](Getting_Started.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzwQpVy95Ghv",
        "colab_type": "text"
      },
      "source": [
        "# Week 1\n",
        "**Plan**\n",
        "- Motivate Machine Learning\n",
        "- Introduce notation used throughout course\n",
        "- Plan for initial lectures\n",
        "    - *What*: Introduce, motivate a model\n",
        "    - *How*:  How to use a model: function signature, code (API)\n",
        "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
        "\n",
        "        \n",
        "- [Course Overview](Course_overview.ipynb)\n",
        "\n",
        "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
        "- [Intro to Classical ML](Intro_Classical_ML.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyzxGhWA5Ghv",
        "colab_type": "text"
      },
      "source": [
        "# Week 2\n",
        "**Plan**\n",
        "- Introduce a model for the Regression task: Linear Regression\n",
        "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
        "\n",
        "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
        "- A *process* for Machine Learning\n",
        "    - Go through the methodical, multi-step process\n",
        "        - Quick first pass, followed by Deeper Dives\n",
        "    - This will be a code-heavy notebook !\n",
        "    - Illustrate Pandas, Jupyter, etc\n",
        "    - [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
        "        - [Linked notebook](Recipe_for_ML.ipynb)\n",
        "\n",
        "**Deeper dives**\n",
        "- Iterative improvement\n",
        "    - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
        "        - Regularization\n",
        "- [Fine tuning techniques](Fine_tuning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aEqTGy25Ghw",
        "colab_type": "text"
      },
      "source": [
        "# Week 3\n",
        "\n",
        "**Plan**\n",
        "- Regression wrapup\n",
        "    - The Loss function for Linear Regression\n",
        "\n",
        "- Recipe for ML focus: Introduction to Transformations (Prepare Data step)\n",
        "    - Transforming data (featuring engineering) is a key step in the Recipe\n",
        "    - We introduce transformations\n",
        "        - Focus on the *how*; subsequent lecture will cover the *why*\n",
        "\n",
        "- Introduce a model for the Classification task: Logistic Regression\n",
        "- How to deal with Categorical (non-numeric) variables\n",
        "    - classification target\n",
        "    - features\n",
        "\n",
        "\n",
        "**Regression wrapup**\n",
        " - [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
        " \n",
        "**Transformations**\n",
        " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
        " \n",
        " \n",
        "**Classification intro**\n",
        "- [Classification: Overview](Classification_Overview.ipynb)\n",
        "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
        "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
        "\n",
        "**Classification, continued**\n",
        "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
        "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
        "\n",
        "**Deeper dives**\n",
        "- [Log odds](Classification_Log_Odds.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HKoXQDu5Ghw",
        "colab_type": "text"
      },
      "source": [
        "# Week 4\n",
        "\n",
        "## Recap of week 3\n",
        "\n",
        "Good news\n",
        "- You now know two main tasks in Supervised Learning\n",
        "    - Regression, Classification\n",
        "- You now know how to use virtually every model in `sklearn`\n",
        "    - Consistent API\n",
        "        - `fit`, `transform`, `predict`\n",
        "- You survived the \"sprint\" to get you up and running with ML\n",
        "- You know the *mechanical process* to implement transformations: Pipelines\n",
        "    \n",
        "**Plan**\n",
        "- Classsification and Categorical variables wrapup\n",
        "    - Baseline models: a baseline model for classification\n",
        "    - OHE and Linear Regression: The Dummy Variable Trap\n",
        "- Error Analysis\n",
        "    - We explain Error Analysis for the Classification Task, with a detailed example\n",
        "    - How Training Loss can be improved\n",
        "- Transformations, continued\n",
        "    - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
        "- Loss functions\n",
        "    - We look at the mathematical logic behind loss functions\n",
        "\n",
        "\n",
        "**Classsification and Categorical variables wrapup**\n",
        "- [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
        "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
        "\n",
        "\n",
        "**Error Analysis**\n",
        "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
        "    - [linked notebook](Error_Analysis.ipynb)\n",
        "        - Summary statistics\n",
        "        - Conditional statistics\n",
        "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
        "\n",
        "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
        " \n",
        "**Transformations, continued**\n",
        "- [Transformations Overview](Transformations_Overview.ipynb)\n",
        "\n",
        "**Loss function**\n",
        "- [Loss functions: the math](Loss_functions.ipynb)\n",
        "    - Maximum likelihood\n",
        "    - Preview: custom loss functions and Deep Learning\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDMfbhlM5Ghx",
        "colab_type": "text"
      },
      "source": [
        "# Week 5\n",
        "\n",
        "**Plan**\n",
        "- More models: Decision Trees, Naive Bayes\n",
        "    - Different flavor: more procedural, less mathematical\n",
        "    - Decision Trees: a model with *non-linear* boundaries\n",
        "- Ensembles\n",
        "    - Bagging and Boosting\n",
        "    - Random Forests\n",
        "- Naive Bayes: a simple but effective model\n",
        "    \n",
        "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
        "\n",
        "**Decision Trees, Ensembles**\n",
        "\n",
        "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
        "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
        "    - [linked notebook](Decision_Trees.ipynb)\n",
        "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
        "\n",
        "**Naive Bayes**\n",
        "- [Naive Bayes](Naive_Bayes.ipynb)\n",
        "\n",
        "**Deeper Dives**\n",
        "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
        "- [Feature importance](Feature_Importance.ipynb)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfV5ad0L5Ghy",
        "colab_type": "text"
      },
      "source": [
        "# Week 6\n",
        "\n",
        "**Plan**\n",
        "\n",
        "- Support Vector Classifiers: a classifier with an interesting twist\n",
        "- Gradient Descent: our tools for solving optimization problems\n",
        "- Interpretation: understanding models\n",
        "\n",
        "\n",
        "**Support Vector Classifiers**\n",
        "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
        "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
        "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)\n",
        "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
        "- [SVM Wrapup](SVM_Coda.ipynb)\n",
        "     \n",
        "**Gradient Descent**\n",
        "- [Gradient Descent](Gradient_Descent.ipynb)\n",
        "\n",
        "**Interpretation**\n",
        "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
        "  \n",
        "        \n",
        "Deeper Dives\n",
        "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
        "- [Missing Data](Missing_Data.ipynb)\n",
        "- [Imbalanced data](Imbalanced_Data.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpJwI3lz5Ghy",
        "colab_type": "text"
      },
      "source": [
        "# Week 7\n",
        "- Plan\n",
        "    - Unsupervised Learning\n",
        "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
        "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
        "    - [linked notebook](Unsupervised.ipynb)\n",
        "    \n",
        "Deeper dives\n",
        "- [Other matrix factorization method](Unsupervised_Other_Factorizations.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jePeQ9pE5Ghz",
        "colab_type": "text"
      },
      "source": [
        "# Week 8 Introduction to Neural Networks and Deep Learning\n",
        "\n",
        "Plan\n",
        "\n",
        "Deep Learning/Neural networks\n",
        "\n",
        "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
        "\n",
        "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)\n",
        "\n",
        "- Coding Neural Networks: Tensorflow, Keras\n",
        "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
        "\n",
        "- Practical Colab\n",
        "    - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/nyutandononline/MachineLearning_and_Finance_Student/blob/master/Colab_practical.ipynb)\n",
        "\n",
        "    \n",
        "Deeper Dives\n",
        "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
        "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
        "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/nyutandononline/MachineLearning_and_Finance_Student/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
        "- [Computation Graphs](Computation_Graphs.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1aqqY2t5Ghz",
        "colab_type": "text"
      },
      "source": [
        "# Week 9 Convolutional Neural Networks\n",
        "\n",
        "Deferred from week 8\n",
        "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
        "\n",
        "Convolutional Neural Networks (CNN)\n",
        "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
        "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
        "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
        "    - [CNN example from github](https://colab.research.google.com/github/nyutandononline/MachineLearning_and_Finance_Student/blob/master/CNN_demo.ipynb) (**Colab**) \n",
        "\n",
        "Deeper dives\n",
        "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n",
        "- [Computation Graphs](Computation_Graphs.ipynb) (**deferred from prior week**)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COka0EPH5Gh0",
        "colab_type": "text"
      },
      "source": [
        "# Week 10 Recurrent Neural Networks\n",
        "\n",
        "Plan\n",
        "- Introduce a new layer type: Recurrent layers\n",
        "    - Part of our \"sprint\": final layer type\n",
        "    - Will revisit more theoretical issues in subsequent lectures\n",
        "    \n",
        "- [Introduction to Recurrent Neural Network (RNN)](Intro_to_RNN.ipynb)\n",
        "- [Recurrent Neural Network Overview](RNN_Overview.ipynb)\n",
        "    - [LSTM_text_generation from github](https://colab.research.google.com/github/nyutandononline/MachineLearning_and_Finance_Student/blob/master/Keras_examples_LSTM_text_generation.ipynb) (**Colab**)\n",
        "\n",
        "\n",
        "Deeper dives\n",
        "- [RNN: How to deal with long sequences](RNN_Long_Sequences.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZDmieew5Gh0",
        "colab_type": "text"
      },
      "source": [
        "# Week 11 Training Neural Networks\n",
        "\n",
        "Plan\n",
        "\n",
        "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
        "\n",
        "Explore the science and art of training a neural network via Gradient Descent.\n",
        "\n",
        "- [Training Neural Networks](Training_Neural_Networks_Overview.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np5InjBD5Gh0",
        "colab_type": "text"
      },
      "source": [
        "# Week 12 Transfer Learning\n",
        "\n",
        "Plan\n",
        "\n",
        "We continue with skills to turn you into a more effective Data Scientist.\n",
        "\n",
        "Our first step is trying to develop intuition for what is occuring within a Neural Network.\n",
        "\n",
        "We will then present an extremely useful trick for leveraging the hard work that others have done.\n",
        "\n",
        "- [Interpretation: preview](Interpretation_preview.ipynb)\n",
        "\n",
        "- [Transfer Learning](Transfer_Learning.ipynb)\n",
        "     - [Transfer Learning example from github](https://colab.research.google.com/github/nyutandononline/MachineLearning_and_Finance_Student/blob/master/TransferLearning_demo.ipynb)(**Colab**)\n",
        "\n",
        "     - [Utility notebook](Dogs_and_Cats_reformat.ipynb)\n",
        "         - Takes the *very large* raw data (from Kaggle) used in the Transfer Learning example\n",
        "         - Creates a much smaller subset, using a different directory structure\n",
        "         - The above notebook uses this reorganized, smaller subset\n",
        "\n",
        "Deeper Dives\n",
        "- [Tensors: Matrix gradients](Matrix_Gradient.ipynb)\n",
        "\n",
        "Further reading\n",
        "- [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhjsyZfa5Gh1",
        "colab_type": "text"
      },
      "source": [
        "# Week 13 Advanced Recurrent Architectures\n",
        "\n",
        "Plan\n",
        "\n",
        "The \"vanilla\" Recurrent Neural Network (RNN) layer we learned is very much exposed to the problem of vanishing/exploding gradients.\n",
        "\n",
        "We will review the issue and demonstrate a related layer type (the LSTM) designed to mitigate the problem.\n",
        "\n",
        "We will also introduce a powerful mechanism called Attention that has recently become quite popular and\n",
        "important.\n",
        "\n",
        "RNN: Issues\n",
        "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
        "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
        "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
        "\n",
        "LSTM: An improved RNN\n",
        "- [Neural Programming](Neural_Programming.ipynb)\n",
        "- [Introduction to the LSTM](Intro_to_LSTM.ipynb)\n",
        "- [LSTM Overview](LSTM_Overview.ipynb)\n",
        "\n",
        "Attention:\n",
        "- [Attention](Intro_to_Attention.ipynb)\n",
        "- [Implementing Attention](Attention_Lookup.ipynb)\n",
        "\n",
        "Further reading\n",
        "- Attention\n",
        "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OI9VNHA5Gh1",
        "colab_type": "text"
      },
      "source": [
        "# Week 14 Advanced Topics\n",
        "\n",
        "Learning from text: Deep Learning for Natural Language Processing (NLP)\n",
        "- [Natural Language Processing Overview](NLP_Overview.ipynb)\n",
        "    - **Colab**: [NLP from github](https://colab.research.google.com/github/nyutandononline/MachineLearning_and_Finance_Student/blob/master/Keras_examples_imdb_cnn.ipynb)\n",
        "\n",
        "What is a Neural Network really doing ? Interpretation\n",
        "- [Introduction to Interpretation of Deep Learning](Intro_to_Interpretation_of_DL.ipynb)\n",
        "- [Interpretation: Simple Methods](Interpretation_of_DL_Simple.ipynb)\n",
        "- [Interpretation: Saliency Maps](Interpretation_of_DL_Deconv.ipynb)\n",
        "- [Interpretation: Gradient Ascent](Interpretation_of_DL_Gradient_Ascent.ipynb)\n",
        "- [Adversarial Examples](Adversarial_Examples.ipynb)\n",
        "\n",
        "Wrapping up\n",
        "- [Final thoughts](Deep_Learning_Coda.ipynb)\n",
        "\n",
        "Deeper Dives\n",
        "- [Interpretation using Principal Components](Interpretation_of_DL_Simple_PCA.ipynb)\n",
        "- [Interpretation: Attention](Interpretation_of_DL_Attention.ipynb)\n",
        "\n",
        "Further reading\n",
        "- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)\n",
        "- [A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HINeNY-D5Gh2",
        "colab_type": "text"
      },
      "source": [
        "# Assignments\n",
        "\n",
        "[Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
        "\n",
        "## Assignment 1\n",
        "- [Assignment 1 notebook](assignments/Assignment_1.ipynb)\n",
        "- [Assignment 1 data](assignments/data/assignment_1)\n",
        "\n",
        "## Assignment 2\n",
        "- [Assignment 2 notebook](assignments/Assignment_2.ipynb)\n",
        "- [Assignment 2 data](assignments/data/assignment_2)\n",
        "\n",
        "## Midterm project\n",
        "- [Midterm project notebook](assignments/Midterm_project.ipynb)\n",
        "- Midterm project data\n",
        "    - [training data](assignments/data/midterm_project/bankruptcy/train/5th_yr.csv)\n",
        "    - [holdout data](assignments/data/midterm_project/bankruptcy/holdout/5th_yr.csv)\n",
        "    \n",
        "## Assignment 3\n",
        "- [Assignment 3 notebook](assignments/Assignment_3.ipynb)\n",
        "\n",
        "## Assignment 4\n",
        "- [Assignment 4 notebook](assignments/Assignment_4.ipynb)\n",
        "\n",
        "## Final project\n",
        "- [Final project guidelines](assignments/Final_project.ipynb)\n",
        "- [Final project: Stock prediction notebook](assignments/Final_project_StockPrediction.ipynb)\n",
        "- Final project data\n",
        "    - [data archive file](https://drive.google.com/file/d/1VpUpxz2syMvKuWZ3is8Qo9XO1cnUum-Y/view?usp=sharing)\n",
        "    - [sample holdout data archive file](https://drive.google.com/file/d/1C-g5CjQkNjHna8OflaNrVo1gfdJqqhUz/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W15N9c4g60b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}