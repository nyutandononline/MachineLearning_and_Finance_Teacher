{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "- Plan\n",
    "    - Motivate Machine Learning\n",
    "    - Introduce notation used throughout course\n",
    "    - Plan for initial lectures\n",
    "        - *What*: Introduce, motivate a model\n",
    "        - *How*:  How to use a model: function signature, code (API)\n",
    "        - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "        \n",
    "        \n",
    "- [Course Overview](Course_overview.ipynb)\n",
    "- [Getting Started](Getting_Started.ipynb)\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "- Plan\n",
    "    - Introduce a model for the Regression task: Linear Regression\n",
    "    - Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "- [Recap: Intro to Classical ML](Recap_of_Intro_Classical_ML.ipynb)\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    - [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "        - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "- The Loss function for Linear Regression\n",
    "    - [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "- Deeper dives\n",
    "    - Iterative improvement\n",
    "        - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "            - Regularization\n",
    "    - [Fine tuning techniques](Fine_tuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "- Deferred from Week 2\n",
    "    - Prepare Data step: Introduction to Transformations\n",
    "        - Transforming data (featuring engineering) is a key step in the Recipe\n",
    "        - We introduce transformations\n",
    "            - Focus on the *how*; subsequent lecture will cover the *why*\n",
    "        - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "        - [Transformation pipelines in `sklearn`](Transformations_Pipelines.ipynb)\n",
    "\n",
    "- Plan\n",
    "    - Introduce a model for the Classification task: Logistic Regression\n",
    "    - How to deal with Categorical (non-numeric) variables\n",
    "        - classification target\n",
    "        - features\n",
    "\n",
    "    - **Classification intro**\n",
    "        - [Classification: Overview](Classification_Overview.ipynb)\n",
    "        - [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "            - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "    - **Non-numeric variables (categorical)**\n",
    "        - [Categorical variables](Categorical_Variables.ipynb)\n",
    "        - [Titanic using categorical features](Classification_and_Non_Numerical_Data.ipynb#Titanic-revisited:-OHE--features)\n",
    "\n",
    "    - **Classification, continued**\n",
    "        - [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "        - [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "\n",
    "- Deeper dives\n",
    "    - [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "    - [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "    - [Log odds](Classification_Log_Odds.ipynb)\n",
    "    - [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "    \n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 2\n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "- Deferred from Week 3\n",
    "    - Deeper dives\n",
    "        - [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "        - [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "        \n",
    "- Plan\n",
    "    - Error Analysis\n",
    "        - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "        - How Training Loss can be improved\n",
    "    - Transformations\n",
    "        - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "    - Loss functions\n",
    "        - We look at the mathematical logic behind loss functions\n",
    "\n",
    "- [Recap and Clarification: Classification](Recap_of_Classification.ipynb)\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [Error Analysis for Classification](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "        - Worked example (Deeper Dive)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "- [Transformations Overview](Transformations_Overview.ipynb)\n",
    "    - [Transformations: the how](Transformations_Overview.ipynb)\n",
    "    - [Transformations: the why](Transformations.ipynb)\n",
    "    \n",
    "- Deeper dives\n",
    "    - [Loss functions: the math](Loss_functions.ipynb)\n",
    "        - Maximum likelihood\n",
    "        - [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "        - Preview: custom loss functions and Deep Learning\n",
    "        \n",
    "    - [Examining errors in MNIST classification](Error_Analysis_MNIST.ipynb)\n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 2\n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "- Plan\n",
    "    - More models: Decision Trees, Naive Bayes\n",
    "        - Different flavor: more procedural, less mathematical\n",
    "        - Decision Trees: a model with *non-linear* boundaries\n",
    "    - Ensembles\n",
    "        - Bagging and Boosting\n",
    "        - Random Forests\n",
    "    - Naive Bayes: a simple but effective model\n",
    "    \n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)   \n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "- Deeper Dives\n",
    "    - [Feature importance](Feature_Importance.ipynb)\n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 4\n",
    "     - [Examining errors in MNIST classification](Error_Analysis_MNIST.ipynb)\n",
    "    - Deferred from week 3  \n",
    "        - [Log odds](Classification_Log_Odds.ipynb)\n",
    "    - Deferred from Week 2 \n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "- Plan\n",
    "    - Support Vector Classifiers: a classifier with an interesting twist\n",
    "    - Interpretation: understanding models\n",
    "    - Gradient Descent: our tools for solving optimization problems\n",
    "    \n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)\n",
    "    \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "     \n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "  \n",
    "        \n",
    "Deeper Dives\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "- [Missing Data](Missing_Data.ipynb)\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 4\n",
    "     - [Examining errors in MNIST classification](Error_Analysis_MNIST.ipynb)\n",
    "    - Deferred from week 3  \n",
    "        - [Log odds](Classification_Log_Odds.ipynb)\n",
    "    - Deferred from Week 2 \n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "- Plan\n",
    "    - Unsupervised Learning\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "    \n",
    "Deeper dives\n",
    "- [Other matrix factorization method](Unsupervised_Other_Factorizations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "Plan\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)\n",
    "\n",
    "- Coding Neural Networks: Tensorflow, Keras\n",
    "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
    "\n",
    "- Practical Colab\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2020/blob/master/Colab_practical.ipynb)\n",
    "-\n",
    "\n",
    "    \n",
    "Deeper Dives\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2020/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Convolutional Neural Networks\n",
    "\n",
    "Deferred from week 8\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "Convolutional Neural Networks (CNN)\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2020/blob/master/CNN_demo.ipynb) (**Colab**) \n",
    "\n",
    "Deeper dives\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb) (**deferred from prior week**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Recurrent Neural Networks\n",
    "\n",
    "Plan\n",
    "- Introduce a new layer type: Recurrent layers\n",
    "    - Part of our \"sprint\": final layer type\n",
    "    - Will revisit more theoretical issues in subsequent lectures\n",
    "    \n",
    "- [Introduction to Recurrent Neural Network (RNN)](Intro_to_RNN.ipynb)\n",
    "- [Recurrent Neural Network Overview](RNN_Overview.ipynb)\n",
    "    - [LSTM_text_generation from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2020/blob/master/Keras_examples_LSTM_text_generation.ipynb) (**Colab**)\n",
    "\n",
    "\n",
    "Deeper dives\n",
    "- [RNN: How to deal with long sequences](RNN_Long_Sequences.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Training Neural Networks\n",
    "\n",
    "Plan\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "Explore the science and art of training a neural network via Gradient Descent.\n",
    "\n",
    "- [Training Neural Networks](Training_Neural_Networks_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Transfer Learning\n",
    "\n",
    "Plan\n",
    "\n",
    "We continue with skills to turn you into a more effective Data Scientist.\n",
    "\n",
    "Our first step is trying to develop intuition for what is occuring within a Neural Network.\n",
    "\n",
    "We will then present an extremely useful trick for leveraging the hard work that others have done.\n",
    "\n",
    "- [Interpretation: preview](Interpretation_preview.ipynb)\n",
    "\n",
    "- [Transfer Learning](Transfer_Learning.ipynb)\n",
    "     - [Transfer Learning example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2020/blob/master/TransferLearning_demo.ipynb)(**Colab**)\n",
    "\n",
    "     - [Utility notebook](Dogs_and_Cats_reformat.ipynb)\n",
    "         - Takes the *very large* raw data (from Kaggle) used in the Transfer Learning example\n",
    "         - Creates a much smaller subset, using a different directory structure\n",
    "         - The above notebook uses this reorganized, smaller subset\n",
    "\n",
    "Deeper Dives\n",
    "- [Tensors: Matrix gradients](Matrix_Gradient.ipynb)\n",
    "\n",
    "Further reading\n",
    "- [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 Advanced Recurrent Architectures\n",
    "\n",
    "Plan\n",
    "\n",
    "The \"vanilla\" Recurrent Neural Network (RNN) layer we learned is very much exposed to the problem of vanishing/exploding gradients.\n",
    "\n",
    "We will review the issue and demonstrate a related layer type (the LSTM) designed to mitigate the problem.\n",
    "\n",
    "We will also introduce a powerful mechanism called Attention that has recently become quite popular and\n",
    "important.\n",
    "\n",
    "RNN: Issues\n",
    "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
    "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
    "\n",
    "LSTM: An improved RNN\n",
    "- [Neural Programming](Neural_Programming.ipynb)\n",
    "- [Introduction to the LSTM](Intro_to_LSTM.ipynb)\n",
    "- [LSTM Overview](LSTM_Overview.ipynb)\n",
    "\n",
    "Attention:\n",
    "- [Attention](Intro_to_Attention.ipynb)\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "\n",
    "Further reading\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 Advanced Topics\n",
    "\n",
    "Learning from text: Deep Learning for Natural Language Processing (NLP)\n",
    "- [Natural Language Processing Overview](NLP_Overview.ipynb)\n",
    "    - **Colab**: [NLP from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2020/blob/master/Keras_examples_imdb_cnn.ipynb)\n",
    "\n",
    "What is a Neural Network really doing ? Interpretation\n",
    "- [Introduction to Interpretation of Deep Learning](Intro_to_Interpretation_of_DL.ipynb)\n",
    "- [Interpretation: Simple Methods](Interpretation_of_DL_Simple.ipynb)\n",
    "- [Interpretation: Saliency Maps](Interpretation_of_DL_Deconv.ipynb)\n",
    "- [Interpretation: Gradient Ascent](Interpretation_of_DL_Gradient_Ascent.ipynb)\n",
    "- [Interpretation: Attention](Interpretation_of_DL_Attention.ipynb)\n",
    "- [Adversarial Examples](Adversarial_Examples.ipynb)\n",
    "\n",
    "Wrapping up\n",
    "- [Final thoughts](Deep_Learning_Coda.ipynb)\n",
    "\n",
    "Deeper Dives\n",
    "- [Interpretation using Principal Components](Interpretation_of_DL_Simple_PCA.ipynb)\n",
    "\n",
    "Further reading\n",
    "- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)\n",
    "- [A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "[Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n",
    "## Assignment 1\n",
    "- [Assignment 1 notebook](assignments/Assignment_1.ipynb)\n",
    "- [Assignment 1 data](assignments/data/assignment_1)\n",
    "\n",
    "## Assignment 2\n",
    "- [Assignment 2 notebook](assignments/Assignment_2.ipynb)\n",
    "- [Assignment 2 data](assignments/data/assignment_2)\n",
    "\n",
    "## Midterm project\n",
    "- [Midterm project notebook](assignments/Midterm_project.ipynb)\n",
    "- Midterm project data\n",
    "    - [training data](assignments/data/midterm_project/bankruptcy/train/5th_yr.csv)\n",
    "    - [holdout data](assignments/data/midterm_project/bankruptcy/holdout/5th_yr.csv)\n",
    "    \n",
    "## Assignment 3\n",
    "- [Assignment 3 notebook](assignments/Assignment_3.ipynb)\n",
    "\n",
    "## Assignment 4\n",
    "- [Assignment 4 notebook](assignments/Assignment_4.ipynb)\n",
    "\n",
    "## Final project\n",
    "- [Final project guidelines](assignments/Final_project.ipynb)\n",
    "- [Final project: Stock prediction notebook](assignments/Final_project_StockPrediction.ipynb)\n",
    "- Final project data\n",
    "    - [data archive file](https://drive.google.com/file/d/1VpUpxz2syMvKuWZ3is8Qo9XO1cnUum-Y/view?usp=sharing)\n",
    "    - [sample holdout data archive file](https://drive.google.com/file/d/1C-g5CjQkNjHna8OflaNrVo1gfdJqqhUz/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
